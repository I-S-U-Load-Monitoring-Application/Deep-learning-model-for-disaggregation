{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30908034",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_appliance = {\n",
    "    'kettle': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 2000,\n",
    "        'max_on_power': 3998,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [10, 8],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'microwave': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 200,\n",
    "        'max_on_power': 3969,\n",
    "        'mean': 500,\n",
    "        'std': 800,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [13, 15],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'fridge': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 50,\n",
    "        'max_on_power': 3323,\n",
    "        'mean': 200,\n",
    "        'std': 400,\n",
    "        's2s_length': 512,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [12, 14],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'dishwasher': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 10,\n",
    "        'max_on_power': 3964,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 1536,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [6, 13],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'washingmachine': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 20,\n",
    "        'max_on_power': 3999,\n",
    "        'mean': 400,\n",
    "        'std': 700,\n",
    "        's2s_length': 2000,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [5, 12],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "DATA_DIRECTORY = 'dataset/ukdale/'\n",
    "SAVE_PATH = 'UKDALE/kettle/'\n",
    "AGG_MEAN = 522\n",
    "AGG_STD = 814\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed97f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:151: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:151: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     ArrowDtype,\n\u001b[0;32m     65\u001b[0m     Int8Dtype,\n\u001b[0;32m     66\u001b[0m     Int16Dtype,\n\u001b[0;32m     67\u001b[0m     Int32Dtype,\n\u001b[0;32m     68\u001b[0m     Int64Dtype,\n\u001b[0;32m     69\u001b[0m     UInt8Dtype,\n\u001b[0;32m     70\u001b[0m     UInt16Dtype,\n\u001b[0;32m     71\u001b[0m     UInt32Dtype,\n\u001b[0;32m     72\u001b[0m     UInt64Dtype,\n\u001b[0;32m     73\u001b[0m     Float32Dtype,\n\u001b[0;32m     74\u001b[0m     Float64Dtype,\n\u001b[0;32m     75\u001b[0m     CategoricalDtype,\n\u001b[0;32m     76\u001b[0m     PeriodDtype,\n\u001b[0;32m     77\u001b[0m     IntervalDtype,\n\u001b[0;32m     78\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     79\u001b[0m     StringDtype,\n\u001b[0;32m     80\u001b[0m     BooleanDtype,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     NA,\n\u001b[0;32m     83\u001b[0m     isna,\n\u001b[0;32m     84\u001b[0m     isnull,\n\u001b[0;32m     85\u001b[0m     notna,\n\u001b[0;32m     86\u001b[0m     notnull,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     Index,\n\u001b[0;32m     89\u001b[0m     CategoricalIndex,\n\u001b[0;32m     90\u001b[0m     RangeIndex,\n\u001b[0;32m     91\u001b[0m     MultiIndex,\n\u001b[0;32m     92\u001b[0m     IntervalIndex,\n\u001b[0;32m     93\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     94\u001b[0m     DatetimeIndex,\n\u001b[0;32m     95\u001b[0m     PeriodIndex,\n\u001b[0;32m     96\u001b[0m     IndexSlice,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     NaT,\n\u001b[0;32m     99\u001b[0m     Period,\n\u001b[0;32m    100\u001b[0m     period_range,\n\u001b[0;32m    101\u001b[0m     Timedelta,\n\u001b[0;32m    102\u001b[0m     timedelta_range,\n\u001b[0;32m    103\u001b[0m     Timestamp,\n\u001b[0;32m    104\u001b[0m     date_range,\n\u001b[0;32m    105\u001b[0m     bdate_range,\n\u001b[0;32m    106\u001b[0m     Interval,\n\u001b[0;32m    107\u001b[0m     interval_range,\n\u001b[0;32m    108\u001b[0m     DateOffset,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     to_numeric,\n\u001b[0;32m    111\u001b[0m     to_datetime,\n\u001b[0;32m    112\u001b[0m     to_timedelta,\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     Flags,\n\u001b[0;32m    115\u001b[0m     Grouper,\n\u001b[0;32m    116\u001b[0m     factorize,\n\u001b[0;32m    117\u001b[0m     unique,\n\u001b[0;32m    118\u001b[0m     value_counts,\n\u001b[0;32m    119\u001b[0m     NamedAgg,\n\u001b[0;32m    120\u001b[0m     array,\n\u001b[0;32m    121\u001b[0m     Categorical,\n\u001b[0;32m    122\u001b[0m     set_eng_float_format,\n\u001b[0;32m    123\u001b[0m     Series,\n\u001b[0;32m    124\u001b[0m     DataFrame,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\core\\api.py:28\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     isna,\n\u001b[0;32m     18\u001b[0m     isnull,\n\u001b[0;32m     19\u001b[0m     notna,\n\u001b[0;32m     20\u001b[0m     notnull,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     factorize,\n\u001b[0;32m     25\u001b[0m     unique,\n\u001b[0;32m     26\u001b[0m     value_counts,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     Float32Dtype,\n\u001b[0;32m     32\u001b[0m     Float64Dtype,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowExtensionArray\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     ExtensionArray,\n\u001b[0;32m      4\u001b[0m     ExtensionOpsMixin,\n\u001b[0;32m      5\u001b[0m     ExtensionScalarOpsMixin,\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanArray\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeArray\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIRECTORY = 'dataset/ukdale/'\n",
    "SAVE_PATH = 'UKDALE/kettle/'\n",
    "AGG_MEAN = 522\n",
    "AGG_STD = 814\n",
    "\n",
    "# def load_dataframe(data_dir, house, channel, col_names=None):\n",
    "#     \"\"\"\n",
    "#     Load a dataframe from the UKDALE dataset.\n",
    "#     :param data_dir: Directory where the UKDALE data is stored.\n",
    "#     :param house: House number to load data from.\n",
    "#     :param channel: Channel number to load data from.\n",
    "#     :param col_names: Optional list of column names for the dataframe.\n",
    "#     :return: DataFrame containing the loaded data.\n",
    "#     \"\"\"\n",
    "#     file_path = f\"{data_dir}house_{house}/channel_{channel}.dat\"\n",
    "#     df = pd.read_csv(file_path, header=None, names=col_names)\n",
    "#     return df\n",
    "\n",
    "def load_dataframe(directory, building, channel, col_names=['time', 'data'], nrows=None):\n",
    "    df = pd.read_table(directory + 'house_' + str(building) + '/' + 'channel_' +\n",
    "                       str(channel) + '.dat',\n",
    "                       sep=\"\\s+\",\n",
    "                       nrows=nrows,\n",
    "                       usecols=[0, 1],\n",
    "                       names=col_names,\n",
    "                       dtype={'time': str},\n",
    "                       )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#def get_arguments():\n",
    "#   parser = argparse.ArgumentParser(description='sequence to point learning \\\n",
    "#                                     example for NILM')\n",
    "#    parser.add_argument('--data_dir', type=str, default=DATA_DIRECTORY,\n",
    "#                          help='The directory containing the UKDALE data')\n",
    "#    parser.add_argument('--appliance_name', type=str, default='kettle',\n",
    "#                          help='which appliance you want to train: kettle,\\\n",
    "#                          microwave,fridge,dishwasher,washingmachine')\n",
    "#   parser.add_argument('--aggregate_mean',type=int,default=AGG_MEAN,\n",
    "#                        help='Mean value of aggregated reading (mains)')\n",
    "#    parser.add_argument('--aggregate_std',type=int,default=AGG_STD,\n",
    "#                        help='Std value of aggregated reading (mains)')\n",
    "#    parser.add_argument('--save_path', type=str, default=SAVE_PATH,\n",
    "#                          help='The directory to store the training data')\n",
    "#    return parser.parse_args()\n",
    "class get_arguments:\n",
    "    def __init__(self):\n",
    "        self.data_dir = DATA_DIRECTORY\n",
    "        self.appliance_name = 'kettle'\n",
    "        self.aggregate_mean = AGG_MEAN\n",
    "        self.aggregate_std = AGG_STD\n",
    "        self.save_path = SAVE_PATH\n",
    "\n",
    "args = get_arguments()\n",
    "appliance_name = args.appliance_name\n",
    "print(appliance_name)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    sample_seconds = 8\n",
    "    training_building_percent = 95\n",
    "    validation_percent = 13\n",
    "    nrows = None\n",
    "    debug = False\n",
    "\n",
    "    train = pd.DataFrame(columns=['aggregate', appliance_name])\n",
    "\n",
    "    for h in params_appliance[appliance_name]['houses']:\n",
    "        print('    ' + args.data_dir + 'house_' + str(h) + '/'\n",
    "              + 'channel_' +\n",
    "              str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "              '.dat')\n",
    "\n",
    "        mains_df = load_dataframe(args.data_dir, h, 1)\n",
    "        app_df = load_dataframe(args.data_dir,\n",
    "                                h,\n",
    "                                params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "                                col_names=['time', appliance_name]\n",
    "                                )\n",
    "\n",
    "        mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
    "        mains_df.set_index('time', inplace=True)\n",
    "        mains_df.columns = ['aggregate']\n",
    "        #resample = mains_df.resample(str(sample_seconds) + 'S').mean()\n",
    "        mains_df.reset_index(inplace=True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"    mains_df:\")\n",
    "            print(mains_df.head())\n",
    "            plt.plot(mains_df['time'], mains_df['aggregate'])\n",
    "            plt.show()\n",
    "\n",
    "        # Appliance\n",
    "        app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n",
    "\n",
    "        if debug:\n",
    "            print(\"app_df:\")\n",
    "            print(app_df.head())\n",
    "            plt.plot(app_df['time'], app_df[appliance_name])\n",
    "            plt.show()\n",
    "\n",
    "        # the timestamps of mains and appliance are not the same, we need to align them\n",
    "        # 1. join the aggragte and appliance dataframes;\n",
    "        # 2. interpolate the missing values;\n",
    "        mains_df.set_index('time', inplace=True)\n",
    "        app_df.set_index('time', inplace=True)\n",
    "\n",
    "        df_align = mains_df.join(app_df, how='outer'). \\\n",
    "            resample(str(sample_seconds) + 'S').mean().fillna(method='backfill', limit=1)\n",
    "        df_align = df_align.dropna()\n",
    "\n",
    "        df_align.reset_index(inplace=True)\n",
    "\n",
    "        del mains_df, app_df, df_align['time']\n",
    "\n",
    "        if debug:\n",
    "            # plot the dtaset\n",
    "            print(\"df_align:\")\n",
    "            print(df_align.head())\n",
    "            plt.plot(df_align['aggregate'].values)\n",
    "            plt.plot(df_align[appliance_name].values)\n",
    "            plt.show()\n",
    "\n",
    "        # Normilization ----------------------------------------------------------------------------------------------\n",
    "        mean = params_appliance[appliance_name]['mean']\n",
    "        std = params_appliance[appliance_name]['std']\n",
    "\n",
    "        df_align['aggregate'] = (df_align['aggregate'] - args.aggregate_mean) / args.aggregate_std\n",
    "        df_align[appliance_name] = (df_align[appliance_name] - mean) / std\n",
    "\n",
    "        if h == params_appliance[appliance_name]['test_build']:\n",
    "            # Test CSV\n",
    "            df_align.to_csv(args.save_path + appliance_name + '_test_.csv', mode='a', index=False, header=False)\n",
    "            print(\"    Size of test set is {:.4f} M rows.\".format(len(df_align) / 10 ** 6))\n",
    "            continue\n",
    "\n",
    "        train = pd.concat([train, df_align], ignore_index=True)\n",
    "        del df_align\n",
    "\n",
    "    # Crop dataset\n",
    "    if training_building_percent is not 0:\n",
    "        train.drop(train.index[-int((len(train)/100)*training_building_percent):], inplace=True)\n",
    "\n",
    "\n",
    "    # Validation CSV\n",
    "    val_len = int((len(train)/100)*validation_percent)\n",
    "    val = train.tail(val_len)\n",
    "    val.reset_index(drop=True, inplace=True)\n",
    "    train.drop(train.index[-val_len:], inplace=True)\n",
    "    # Validation CSV\n",
    "    val.to_csv(args.save_path + appliance_name + '_validation_' + '.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    # Training CSV\n",
    "    train.to_csv(args.save_path + appliance_name + '_training_.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    print(\"    Size of total training set is {:.4f} M rows.\".format(len(train) / 10 ** 6))\n",
    "    print(\"    Size of total validation set is {:.4f} M rows.\".format(len(val) / 10 ** 6))\n",
    "    del train, val\n",
    "\n",
    "\n",
    "    print(\"\\nPlease find files in: \" + args.save_path)\n",
    "    print(\"Total elapsed time: {:.2f} min.\".format((time.time() - start_time) / 60))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5e5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kettle\n",
      "Starting creating testset...\n",
      "dataset/ukdale/house_1/channel_10.dat\n",
      "         time  kettle\n",
      "0  1352500098       1\n",
      "1  1352500104       1\n",
      "2  1352500110       1\n",
      "3  1352500116       1\n",
      "4  1352500122       1\n",
      "         time  kettle\n",
      "0  1352500095     599\n",
      "1  1352500101     582\n",
      "2  1352500107     600\n",
      "3  1352500113     586\n",
      "4  1352500120     596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:64: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  kettle\n",
      "0 1970-01-16 15:41:40.095     599\n",
      "1 1970-01-16 15:41:40.101     582\n",
      "2 1970-01-16 15:41:40.107     600\n",
      "3 1970-01-16 15:41:40.113     586\n",
      "4 1970-01-16 15:41:40.120     596\n",
      "                     time  kettle\n",
      "0 1970-01-16 15:41:40.098       1\n",
      "1 1970-01-16 15:41:40.104       1\n",
      "2 1970-01-16 15:41:40.110       1\n",
      "3 1970-01-16 15:41:40.116       1\n",
      "4 1970-01-16 15:41:40.122       1\n",
      "   aggregate  kettle\n",
      "0        599       1\n",
      "1        582       1\n",
      "2        600       1\n",
      "3        586       1\n",
      "4        596       1\n",
      "                     aggregate  kettle\n",
      "1970-01-01 00:00:00      590.5     1.0\n",
      "1970-01-01 00:00:08      600.0     1.0\n",
      "1970-01-01 00:00:16      586.0     1.0\n",
      "1970-01-01 00:00:24      588.5     1.0\n",
      "1970-01-01 00:00:32      597.0     1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:79: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:81: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample = df.resample('8S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set is 0.075 M rows (House 1).\n",
      "dataset/ukdale/house_2/channel_8.dat\n",
      "         time  kettle\n",
      "0  1361116822       0\n",
      "1  1361116825       0\n",
      "2  1361116831       0\n",
      "3  1361116837       0\n",
      "4  1361116843       0\n",
      "         time  kettle\n",
      "0  1361117854     340\n",
      "1  1361117860     341\n",
      "2  1361117866     347\n",
      "3  1361117872     350\n",
      "4  1361117878     342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:64: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:65: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  kettle\n",
      "0 1970-01-16 18:05:17.854     340\n",
      "1 1970-01-16 18:05:17.860     341\n",
      "2 1970-01-16 18:05:17.866     347\n",
      "3 1970-01-16 18:05:17.872     350\n",
      "4 1970-01-16 18:05:17.878     342\n",
      "                     time  kettle\n",
      "0 1970-01-16 18:05:16.822       0\n",
      "1 1970-01-16 18:05:16.825       0\n",
      "2 1970-01-16 18:05:16.831       0\n",
      "3 1970-01-16 18:05:16.837       0\n",
      "4 1970-01-16 18:05:16.843       0\n",
      "   aggregate  kettle\n",
      "0        340       0\n",
      "1        341       0\n",
      "2        347       0\n",
      "3        350       0\n",
      "4        342       0\n",
      "                     aggregate  kettle\n",
      "1970-01-01 00:00:00      340.5     0.0\n",
      "1970-01-01 00:00:08      347.0     0.0\n",
      "1970-01-01 00:00:16      350.0     0.0\n",
      "1970-01-01 00:00:24      341.5     0.0\n",
      "1970-01-01 00:00:32      343.0     0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:79: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14180\\1025023502.py:81: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample = df.resample('8S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set is 0.075 M rows (House 2).\n",
      "\n",
      "Normalization parameters: \n",
      "Mean and standard deviation values USED for AGGREGATE are:\n",
      "    Mean = 522, STD = 814\n",
      "Mean and standard deviation values USED for kettle are:\n",
      "    Mean = 700, STD = 1000\n",
      "\n",
      "Please find files in: UKDALE/kettle/\n",
      "\n",
      "Total elapsed time: 0 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "appliance_name = 'kettle'\n",
    "print(appliance_name)\n",
    "\n",
    "# UK-DALE path\n",
    "path = 'dataset/ukdale/'\n",
    "save_path = 'UKDALE/kettle/'\n",
    "\n",
    "aggregate_mean = 522\n",
    "aggregate_std = 814\n",
    "\n",
    "nrows = 10**5\n",
    "\n",
    "\n",
    "\n",
    "def load(path, building, appliance, channel, nrows=None):\n",
    "    # load csv\n",
    "    file_name = path + 'house_' + str(building) + '/' + 'channel_' + str(channel) + '.dat'\n",
    "    single_csv = pd.read_csv(file_name,\n",
    "                             sep=' ',\n",
    "                             #header=0,\n",
    "                             names=['time', appliance],\n",
    "                             dtype={'time': str, \"appliance\": int},\n",
    "                             #parse_dates=['time'],\n",
    "                             #date_parser=pd.to_datetime,\n",
    "                             nrows=nrows,\n",
    "                             usecols=[0, 1],\n",
    "                             engine='python'\n",
    "                             )\n",
    "    return single_csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting creating testset...\")\n",
    "\n",
    "for h in params_appliance[appliance_name]['houses']:\n",
    "\n",
    "    print(path + 'house_' + str(h) + '/'\n",
    "          + 'channel_' +\n",
    "          str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "          '.dat')\n",
    "\n",
    "    agg_df = load(path,\n",
    "                  h,\n",
    "                  appliance_name,\n",
    "                  1,\n",
    "                  nrows=nrows,\n",
    "                  )\n",
    "\n",
    "    df = load(path,\n",
    "              h,\n",
    "              appliance_name,\n",
    "              params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "              nrows=nrows,\n",
    "              )\n",
    "\n",
    "    #for i in range(100):\n",
    "    #    print(int(df['time'][i]) - int(agg_df['time'][i]))\n",
    "\n",
    "    # Time conversion\n",
    "    print(df.head())\n",
    "    print(agg_df.head())\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n",
    "    print(agg_df.head())\n",
    "    print(df.head())\n",
    "\n",
    "    df['aggregate'] = agg_df[appliance_name]\n",
    "    cols = df.columns.tolist()\n",
    "    del cols[0]\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    # Re-sampling\n",
    "    ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
    "    df.set_index(ind, inplace=True, drop=True)\n",
    "    resample = df.resample('8S')\n",
    "    df = resample.mean()\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    # Normalization\n",
    "    df['aggregate'] = (df['aggregate'] - aggregate_mean) / aggregate_std\n",
    "    df[appliance_name] = \\\n",
    "        (df[appliance_name] - params_appliance[appliance_name]['mean']) / params_appliance[appliance_name]['std']\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(save_path + appliance_name + '_test_' + 'uk-dale_' + 'H' + str(h) + '.csv', index=False)\n",
    "\n",
    "    print(\"Size of test set is {:.3f} M rows (House {:d}).\"\n",
    "          .format(df.shape[0] / 10 ** 6, h))\n",
    "\n",
    "    del df\n",
    "\n",
    "\n",
    "print(\"\\nNormalization parameters: \")\n",
    "print(\"Mean and standard deviation values USED for AGGREGATE are:\")\n",
    "print(\"    Mean = {:d}, STD = {:d}\".format(aggregate_mean, aggregate_std))\n",
    "\n",
    "print('Mean and standard deviation values USED for ' + appliance_name + ' are:')\n",
    "print(\"    Mean = {:d}, STD = {:d}\"\n",
    "      .format(params_appliance[appliance_name]['mean'], params_appliance[appliance_name]['std']))\n",
    "\n",
    "print(\"\\nPlease find files in: \" + save_path)\n",
    "tot = int(int(time.time() - start_time) / 60)\n",
    "print(\"\\nTotal elapsed time: \" + str(tot) + ' min')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5b798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 02:57:53,421 [INFO ]  Parameters: \n",
      "2025-06-20 02:57:53,422 [INFO ]  Machine name: DESKTOP-N5CITIQ\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import socket\n",
    "\n",
    "log_file_name = '{}.log'.format(time.strftime(\"%Y-%m-%d-%H:%M:%S\").replace(':','-'))\n",
    "\n",
    "with open(log_file_name, 'w'):\n",
    "    pass\n",
    "\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(levelname)-5.5s]  %(message)s\")\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.DEBUG)\n",
    "fileHandler = logging.FileHandler(\"{0}\".format(log_file_name))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(consoleHandler)\n",
    "\n",
    "\n",
    "def log(string, level='info'):\n",
    "\n",
    "    if level == 'info':\n",
    "        rootLogger.info(string)\n",
    "    elif level == 'debug':\n",
    "        rootLogger.debug(string)\n",
    "    elif level == 'warning':\n",
    "        rootLogger.warning(string)\n",
    "\n",
    "log('Parameters: ')\n",
    "machine_name = socket.gethostname()\n",
    "log('Machine name: ' + machine_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9e060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Reshape\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_model(appliance, input_tensor, window_length, transfer_dense=False, transfer_cnn=False,\n",
    "              cnn='kettle', n_dense=1, pretrainedmodel_dir='saved_models/kettle_best_model'):\n",
    "\n",
    "    reshape = Reshape((-1, window_length, 1),\n",
    "                      )(input_tensor)\n",
    "\n",
    "    cnn1 = Conv2D(filters=30,\n",
    "                  kernel_size=(10, 1),\n",
    "                  strides=(1, 1),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  )(reshape)\n",
    "\n",
    "    cnn2 = Conv2D(filters=30,\n",
    "                  kernel_size=(8, 1),\n",
    "                  strides=(1, 1),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  )(cnn1)\n",
    "\n",
    "    cnn3 = Conv2D(filters=40,\n",
    "                  kernel_size=(6, 1),\n",
    "                  strides=(1, 1),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  )(cnn2)\n",
    "\n",
    "    cnn4 = Conv2D(filters=50,\n",
    "                  kernel_size=(5, 1),\n",
    "                  strides=(1, 1),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  )(cnn3)\n",
    "\n",
    "    cnn5 = Conv2D(filters=50,\n",
    "                  kernel_size=(5, 1),\n",
    "                  strides=(1, 1),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  )(cnn4)\n",
    "\n",
    "    flat = Flatten(name='flatten')(cnn5)\n",
    "\n",
    "    d = Dense(1024, activation='relu', name='dense')(flat)\n",
    "\n",
    "    if n_dense == 1:\n",
    "        label = d\n",
    "    elif n_dense == 2:\n",
    "        d1 = Dense(1024, activation='relu', name='dense1')(d)\n",
    "        label = d1\n",
    "    elif n_dense == 3:\n",
    "        d1 = Dense(1024, activation='relu', name='dense1')(d)\n",
    "        d2 = Dense(1024, activation='relu', name='dense2')(d1)\n",
    "        label = d2\n",
    "\n",
    "    d_out = Dense(1, activation='linear', name='output')(label)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=d_out)\n",
    "\n",
    "    session = K.get_session()\n",
    "\n",
    "    if transfer_dense:\n",
    "        log(\"Transfer learning...\")\n",
    "        log(\"...loading an entire pre-trained model\")\n",
    "        weights_loader(model, pretrainedmodel_dir+'/cnn_s2p_' + appliance + '_pointnet_model')\n",
    "        model_def = model\n",
    "    elif transfer_cnn and not transfer_dense:\n",
    "        log(\"Transfer learning...\")\n",
    "        log('...loading a ' + appliance + ' pre-trained-cnn')\n",
    "        cnn_weights_loader(model, cnn, pretrainedmodel_dir)\n",
    "        model_def = model\n",
    "        for idx, layer1 in enumerate(model_def.layers):\n",
    "            if hasattr(layer1, 'kernel_initializer') and 'conv2d' not in layer1.name and 'cnn' not in layer1.name:\n",
    "                log('Re-initialize: {}'.format(layer1.name))\n",
    "                layer1.kernel.initializer.run(session=session)\n",
    "\n",
    "    elif not transfer_dense and not transfer_cnn:\n",
    "        log(\"Standard training...\")\n",
    "        log(\"...creating a new model.\")\n",
    "        model_def = model\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Model selection error.')\n",
    "\n",
    "    # Printing, logging and plotting the model\n",
    "    model_def.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')\n",
    "    #plot_model(model_def, to_file='model_def.png', show_shapes=True, show_layer_names=True, rankdir='TB')\n",
    "\n",
    "    # Adding network structure to both the log file and output terminal\n",
    "    files = [x for x in os.listdir('./') if x.endswith(\".log\")]\n",
    "    with open(max(files, key=os.path.getctime), 'a') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model_def.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # Check weights slice\n",
    "    for v in tf.compat.v1.trainable_variables():\n",
    "        if v.name == 'conv2d_1/kernel:0':\n",
    "            cnn1_weights = session.run(v)\n",
    "    return model_def, cnn1_weights\n",
    "\n",
    "\n",
    "def print_attrs(name, obj):\n",
    "    print(name)\n",
    "    for key, val in obj.attrs.items():\n",
    "        print(\"    %s: %s\" % (key, val))\n",
    "\n",
    "\n",
    "def cnn_weights_loader(model_to_fill, cnn_appliance, pretrainedmodel_dir):\n",
    "    log('Loading cnn weights from ' + cnn_appliance)    \n",
    "    weights_path = pretrainedmodel_dir+'/cnn_s2p_' + cnn_appliance + '_pointnet_model' + '_weights.h5'\n",
    "    if not os.path.exists(weights_path):\n",
    "        print('The directory does not exist or you do not have the files for trained model')\n",
    "        \n",
    "    f = h5py.File(weights_path, 'r')\n",
    "    log(f.visititems(print_attrs))\n",
    "    layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n",
    "    for name in layer_names:\n",
    "        if 'conv2d_' in name or 'cnn' in name:\n",
    "            g = f[name]\n",
    "            weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n",
    "            if len(weight_names):\n",
    "                weight_values = [g[weight_name] for weight_name in weight_names]\n",
    "\n",
    "            model_to_fill.layers[int(name[-1])+1].set_weights(weight_values)\n",
    "            log('Loaded cnn layer: {}'.format(name))\n",
    "\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "\n",
    "\n",
    "def weights_loader(model, path):\n",
    "    log('Loading cnn weights from ' + path)\n",
    "    model.load_weights(path + '_weights.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9251467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkDoubleSourceSlider(object):\n",
    "    def __init__(self, filename, batchsize, chunksize, shuffle, offset, crop=None, header=0, ram_threshold=5*10**5):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.batchsize = batchsize\n",
    "        self.chunksize = chunksize\n",
    "        self.shuffle = shuffle\n",
    "        self.offset = offset\n",
    "        self.header = header\n",
    "        self.crop = crop\n",
    "        self.ram = ram_threshold\n",
    "\n",
    "    def check_lenght(self):\n",
    "        # check the csv size\n",
    "        check_cvs = pd.read_csv(self.filename,\n",
    "                                nrows=self.crop,\n",
    "                                chunksize=10 ** 3,\n",
    "                                header=self.header\n",
    "                                )\n",
    "\n",
    "        t_size = 0\n",
    "\n",
    "        for chunk in check_cvs:\n",
    "            size = chunk.shape[0]\n",
    "            t_size += size\n",
    "            del chunk\n",
    "        log('Size of the dataset is {:.3f} M rows.'.format(t_size/10 ** 6))\n",
    "        if t_size > self.ram:  # IF dataset is too large for memory\n",
    "            log('It is too large to fit in memory so it will be loaded in chunkes of size {:}.'.format(self.chunksize))\n",
    "        else:\n",
    "            log('This size can fit the memory so it will load entirely')\n",
    "\n",
    "        return t_size\n",
    "\n",
    "    def feed_chunk(self):\n",
    "\n",
    "        try:\n",
    "            total_size\n",
    "        except NameError:\n",
    "            #global total_size\n",
    "            total_size = ChunkDoubleSourceSlider.check_lenght(self)\n",
    "\n",
    "        if total_size > self.ram:  # IF dataset is too large for memory\n",
    "\n",
    "            # LOAD data from csv\n",
    "            data_frame = pd.read_csv(self.filename,\n",
    "                                     nrows=self.crop,\n",
    "                                     chunksize=self.chunksize,\n",
    "                                     header=self.header\n",
    "                                     )\n",
    "\n",
    "            # iterations over csv file\n",
    "            for chunk in data_frame:\n",
    "\n",
    "                np_array = np.array(chunk)\n",
    "                inputs, targets = np_array[:, 0], np_array[:, 1]\n",
    "\n",
    "                \"\"\"\n",
    "                if len(inputs) < self.batchsize:\n",
    "                    while len(inputs) == self.batchsize:\n",
    "                        inputs = np.append(inputs, 0)\n",
    "                       targets = np.append(targets, 0)\n",
    "                \"\"\"\n",
    "\n",
    "                max_batchsize = inputs.size - 2 * self.offset\n",
    "                if self.batchsize < 0:\n",
    "                    self.batchsize = max_batchsize\n",
    "\n",
    "                # define indices and shuffle them if necessary\n",
    "                indices = np.arange(max_batchsize)\n",
    "                if self.shuffle:\n",
    "                    np.random.shuffle(indices)\n",
    "\n",
    "                # providing sliding windows:\n",
    "                for start_idx in range(0, max_batchsize, self.batchsize):\n",
    "\n",
    "                    excerpt = indices[start_idx:start_idx + self.batchsize]\n",
    "\n",
    "                    inp = np.array([inputs[idx:idx + 2 * self.offset + 1] for idx in excerpt])\n",
    "                    tar = targets[excerpt + self.offset].reshape(-1, 1)\n",
    "\n",
    "                    yield inp, tar\n",
    "\n",
    "        else:  # IF dataset can fit the memory\n",
    "\n",
    "            # LOAD data from csv\n",
    "            data_frame = pd.read_csv(self.filename,\n",
    "                                     nrows=self.crop,\n",
    "                                     header=self.header\n",
    "                                     )\n",
    "\n",
    "            np_array = np.array(data_frame)\n",
    "            inputs, targets = np_array[:, 0], np_array[:, 1]\n",
    "\n",
    "\n",
    "            max_batchsize = inputs.size - 2 * self.offset\n",
    "            if self.batchsize < 0:\n",
    "                    self.batchsize = max_batchsize\n",
    "\n",
    "            # define indices and shuffle them if necessary\n",
    "            indices = np.arange(max_batchsize)\n",
    "            if self.shuffle:\n",
    "                    np.random.shuffle(indices)\n",
    "\n",
    "            # providing sliding windows:\n",
    "            for start_idx in range(0, max_batchsize, self.batchsize):\n",
    "                excerpt = indices[start_idx:start_idx + self.batchsize]\n",
    "\n",
    "                inp = np.array([inputs[idx:idx + 2 * self.offset + 1] for idx in excerpt])\n",
    "                tar = targets[excerpt + self.offset].reshape(-1, 1)\n",
    "\n",
    "                yield inp, tar\n",
    "\n",
    "\n",
    "class ChunkDoubleSourceSlider2(object):\n",
    "    def __init__(self, filename, batchsize, chunksize, shuffle, offset, crop=None, header=0, ram_threshold=5 * 10 ** 5):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.batchsize = batchsize\n",
    "        self.chunksize = chunksize\n",
    "        self.shuffle = shuffle\n",
    "        self.offset = offset\n",
    "        self.header = header\n",
    "        self.crop = crop\n",
    "        self.ram = ram_threshold\n",
    "        self.total_size = 0\n",
    "\n",
    "    def check_length(self):\n",
    "        # check the csv size\n",
    "        check_cvs = pd.read_csv(self.filename,\n",
    "                                nrows=self.crop,\n",
    "                                chunksize=10 ** 3,\n",
    "                                header=self.header\n",
    "                                )\n",
    "\n",
    "        for chunk in check_cvs:\n",
    "            size = chunk.shape[0]\n",
    "            self.total_size += size\n",
    "            del chunk\n",
    "        log('Size of the dataset is {:.3f} M rows.'.format(self.total_size / 10 ** 6))\n",
    "        if self.total_size > self.ram:  # IF dataset is too large for memory\n",
    "            log('It is too large to fit in memory so it will be loaded in chunkes of size {:}.'.format(self.chunksize))\n",
    "        else:\n",
    "            log('This size can fit the memory so it will load entirely')\n",
    "\n",
    "    def feed_chunk(self):\n",
    "\n",
    "        if self.total_size == 0:\n",
    "            ChunkDoubleSourceSlider2.check_length(self)\n",
    "\n",
    "        if self.total_size > self.ram:  # IF dataset is too large for memory\n",
    "\n",
    "            # LOAD data from csv\n",
    "            data_frame = pd.read_csv(self.filename,\n",
    "                                     nrows=self.crop,\n",
    "                                     chunksize=self.chunksize,\n",
    "                                     header=self.header\n",
    "                                     )\n",
    "\n",
    "            skip_idx = np.arange(self.total_size/self.chunksize)\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(skip_idx)\n",
    "\n",
    "            log(str(skip_idx), 'debug')\n",
    "\n",
    "            for i in skip_idx:\n",
    "\n",
    "                log('index: ' + str(i), 'debug')\n",
    "\n",
    "                # Read the data\n",
    "                data = pd.read_csv(self.filename,\n",
    "                                   nrows=self.chunksize,\n",
    "                                   skiprows=int(i)*self.chunksize,\n",
    "                                   header=self.header)\n",
    "\n",
    "                np_array = np.array(data)\n",
    "                inputs, targets = np_array[:, 0], np_array[:, 1]\n",
    "\n",
    "                max_batchsize = inputs.size - 2 * self.offset\n",
    "                if self.batchsize < 0:\n",
    "                    self.batchsize = max_batchsize\n",
    "\n",
    "                # define indices and shuffle them if necessary\n",
    "                indices = np.arange(max_batchsize)\n",
    "                if self.shuffle:\n",
    "                    np.random.shuffle(indices)\n",
    "\n",
    "                # providing sliding windows:\n",
    "                for start_idx in range(0, max_batchsize, self.batchsize):\n",
    "                    excerpt = indices[start_idx:start_idx + self.batchsize]\n",
    "\n",
    "                    inp = np.array([inputs[idx:idx + 2 * self.offset + 1] for idx in excerpt])\n",
    "                    tar = targets[excerpt + self.offset].reshape(-1, 1)\n",
    "\n",
    "                    yield inp, tar\n",
    "\n",
    "        else:  # IF dataset can fit the memory\n",
    "\n",
    "            # LOAD data from csv\n",
    "            data_frame = pd.read_csv(self.filename,\n",
    "                                     nrows=self.crop,\n",
    "                                     header=self.header\n",
    "                                     )\n",
    "\n",
    "            np_array = np.array(data_frame)\n",
    "            inputs, targets = np_array[:, 0], np_array[:, 1]\n",
    "\n",
    "            max_batchsize = inputs.size - 2 * self.offset\n",
    "            if self.batchsize < 0:\n",
    "                self.batchsize = max_batchsize\n",
    "\n",
    "            # define indices and shuffle them if necessary\n",
    "            indices = np.arange(max_batchsize)\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "            # providing sliding windows:\n",
    "            for start_idx in range(0, max_batchsize, self.batchsize):\n",
    "                excerpt = indices[start_idx:start_idx + self.batchsize]\n",
    "\n",
    "                inp = np.array([inputs[idx:idx + 2 * self.offset + 1] for idx in excerpt])\n",
    "                tar = targets[excerpt + self.offset].reshape(-1, 1)\n",
    "\n",
    "                yield inp, tar\n",
    "\n",
    "\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Loads the entire dataset from the CSV file and returns X, y numpy arrays.\n",
    "        Assumes the last column is the target.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        data = pd.read_csv(self.filename, header=self.header)\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class DoubleSourceProvider2(object):\n",
    "\n",
    "    def __init__(self, batchsize, shuffle, offset):\n",
    "\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "        self.offset = offset\n",
    "\n",
    "    def feed(self, inputs, targets):\n",
    "\n",
    "        assert len(inputs) == len(targets)\n",
    "\n",
    "        inputs = inputs.flatten()\n",
    "        targets = targets.flatten()\n",
    "\n",
    "        max_batchsize = inputs.size - 2 * self.offset\n",
    "\n",
    "        if self.batchsize == -1:\n",
    "            self.batchsize = len(inputs)\n",
    "\n",
    "        indices = np.arange(max_batchsize)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for start_idx in range(0, max_batchsize, self.batchsize):\n",
    "            excerpt = indices[start_idx:start_idx + self.batchsize]\n",
    "\n",
    "            yield np.array([inputs[idx:idx + 2 * self.offset + 1] for idx in excerpt]),\\\n",
    "                  targets[excerpt + self.offset].reshape(-1, 1)\n",
    "\n",
    "\n",
    "class DoubleSourceProvider3(object):\n",
    "\n",
    "    def __init__(self, nofWindows, offset):\n",
    "\n",
    "        self.nofWindows = nofWindows\n",
    "        self.offset = offset\n",
    "\n",
    "    def feed(self, inputs):\n",
    "\n",
    "        inputs = inputs.flatten()\n",
    "        max_nofw = inputs.size - 2 * self.offset\n",
    "\n",
    "        if self.nofWindows < 0:\n",
    "            self.nofWindows = max_nofw\n",
    "\n",
    "        indices = np.arange(max_nofw, dtype=int)\n",
    "\n",
    "        # providing sliding windows:\n",
    "        for start_idx in range(0, max_nofw, self.nofWindows):\n",
    "            excerpt = indices[start_idx:start_idx + self.nofWindows]\n",
    "\n",
    "            inp = np.array([inputs[idx:idx + 2 * self.offset + 1] for idx in excerpt])\n",
    "\n",
    "            yield inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fe7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dict_to_one(dp_dict={}):\n",
    "\n",
    "    \"\"\" Input a dictionary, return a dictionary that all items are\n",
    "    set to one, use for disable dropout, drop-connect layer and so on.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dp_dict : dictionary keeping probabilities date\n",
    "    \"\"\"\n",
    "    return {x: 1 for x in dp_dict}\n",
    "\n",
    "\n",
    "def modelsaver(network, path, epoch_identifier=None):\n",
    "\n",
    "    if epoch_identifier:\n",
    "        ifile = path + '_' + str(epoch_identifier)\n",
    "    else:\n",
    "        ifile = path\n",
    "\n",
    "    network.save(ifile + '.h5')\n",
    "    network.save_weights(ifile + '_weights' + '.h5')\n",
    "\n",
    "\n",
    "def customfit(sess,\n",
    "              network,\n",
    "              cost,\n",
    "              train_op,\n",
    "              tra_provider,\n",
    "              x,\n",
    "              y_,\n",
    "              acc=None,\n",
    "              n_epoch=50,\n",
    "              print_freq=1,\n",
    "              val_provider=None,\n",
    "              save_model=-1,\n",
    "              tra_kwag=None,\n",
    "              val_kwag=None,\n",
    "              save_path=None,\n",
    "              epoch_identifier=None,\n",
    "              earlystopping=True,\n",
    "              min_epoch=1,\n",
    "              patience=10):\n",
    "    \"\"\"\n",
    "        Traing a given network by the given cost function, dataset, n_epoch etc.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sess : TensorFlow session\n",
    "            sess = tf.InteractiveSession()\n",
    "        network : a TensorLayer layer\n",
    "            the network will be trained\n",
    "        train_op : a TensorFlow optimizer\n",
    "            like tf.train.AdamOptimizer\n",
    "        x : placeholder\n",
    "            for inputs\n",
    "        y_ : placeholder\n",
    "            for targets\n",
    "        acc : the TensorFlow expression of accuracy (or other metric) or None\n",
    "            if None, would not display the metric\n",
    "        batch_size : int\n",
    "            batch size for training and evaluating\n",
    "        n_epoch : int\n",
    "            the number of training epochs\n",
    "        print_freq : int\n",
    "            display the training information every ``print_freq`` epochs\n",
    "        X_val : numpy array or None\n",
    "            the input of validation data\n",
    "        y_val : numpy array or None\n",
    "            the target of validation data\n",
    "        eval_train : boolen\n",
    "            if X_val and y_val are not None, it refects whether to evaluate the training data\n",
    "    \"\"\"\n",
    "    # parameters for earlystopping\n",
    "    best_valid = np.inf\n",
    "    best_valid_acc = np.inf\n",
    "    best_valid_epoch = min_epoch\n",
    "\n",
    "    # Training info\n",
    "    total_train_loss = []\n",
    "    total_val_loss = []\n",
    "    single_step_train_loss = []\n",
    "    single_step_val_loss = []\n",
    "\n",
    "    log(\"Start training the network ...\")\n",
    "    start_time_begin = time.time()\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        loss_ep = 0\n",
    "        n_step = 0\n",
    "        log(\"------------------------- Epoch %d of %d --------------------------\" % (epoch + 1, n_epoch))\n",
    "\n",
    "        for batch in tra_provider.feed_chunk():\n",
    "\n",
    "            X_train_a, y_train_a = batch\n",
    "            X_train_a = K.cast_to_floatx(X_train_a)\n",
    "            y_train_a = K.cast_to_floatx(y_train_a)\n",
    "\n",
    "            feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "            #feed_dict.update(network.all_drop)  # enable noise layers\n",
    "            loss, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n",
    "            loss_ep += loss\n",
    "            n_step += 1\n",
    "            #print(\"    batch {0:d}\".format(n_step))\n",
    "            #log(tf.trainable_variables())\n",
    "\n",
    "            \"\"\"\n",
    "            for v in tf.trainable_variables():\n",
    "                if v.name == 'conv2d_1/kernel:0':\n",
    "                    value = sess.run(v)\n",
    "                    print(value)\n",
    "                    break\n",
    "            \"\"\"\n",
    "\n",
    "            #for k, v in zip(variables_names, values):\n",
    "            #   print(k, v)\n",
    "        loss_ep = loss_ep / n_step\n",
    "        log('loss_ep: %f' % loss_ep)\n",
    "\n",
    "        if epoch >= 0 or (epoch + 1) % print_freq == 0:\n",
    "            # evaluate the val error at each epoch.\n",
    "            if val_provider is not None:\n",
    "                log(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "                log(\"Validation...\")\n",
    "                train_loss, train_acc, n_batch_train = 0, 0, 0\n",
    "                for batch in tra_provider.feed_chunk():\n",
    "                    X_train_a, y_train_a = batch\n",
    "                    #dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
    "                    feed_dict = {x: X_train_a, y_: y_train_a}\n",
    "                    #feed_dict.update(dp_dict)\n",
    "                    if acc is not None:\n",
    "                        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                        train_acc += ac\n",
    "                    else:\n",
    "                        err = sess.run(cost, feed_dict=feed_dict)\n",
    "                    train_loss += err\n",
    "                    n_batch_train += 1\n",
    "                    single_step_train_loss.append(err)\n",
    "                total_train_loss.append(train_loss/n_batch_train)\n",
    "                log(\"   train loss/n_batch_train: %f\" % (train_loss / n_batch_train))\n",
    "                log(\"   train loss: %f, n_batch_train: %d\" % (train_loss, n_batch_train))\n",
    "\n",
    "                if acc is not None:\n",
    "                    log(\"   train acc: %f\" % (train_acc / n_batch_train))\n",
    "\n",
    "                val_loss, val_acc, n_batch_val = 0, 0, 0\n",
    "\n",
    "                for batch in val_provider.feed_chunk():\n",
    "                    X_val_a, y_val_a = batch\n",
    "                    #dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
    "                    feed_dict = {x: X_val_a, y_: y_val_a}\n",
    "                    #feed_dict.update(dp_dict)\n",
    "                    if acc is not None:\n",
    "                        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                        val_acc += ac\n",
    "                    else:\n",
    "                        err = sess.run(cost, feed_dict=feed_dict)\n",
    "                    val_loss += err\n",
    "                    n_batch_val += 1\n",
    "                    single_step_val_loss.append(err)\n",
    "                log(\"    val loss: %f\" % (val_loss / n_batch_val))\n",
    "                total_val_loss.append(val_loss/n_batch_val)\n",
    "                if acc is not None:\n",
    "                    log(\"   val acc: %f\" % (val_acc / n_batch_val))\n",
    "            else:\n",
    "                log('no validation')\n",
    "                log(\"Epoch %d of %d took %fs, loss %f\" % (epoch + 1, n_epoch, time.time() - start_time, loss_ep))\n",
    "\n",
    "        if earlystopping:\n",
    "            if epoch >= min_epoch:\n",
    "                log(\"Evaluate earlystopping parameters...\")\n",
    "                current_valid = val_loss / n_batch_val\n",
    "                current_valid_acc = val_acc / n_batch_val\n",
    "                current_epoch = epoch\n",
    "                current_train_loss = train_loss / n_batch_train\n",
    "                current_train_acc = train_acc / n_batch_train\n",
    "                log('    Current valid loss was {:.6f}, acc was {:.6f}, '\n",
    "                    'train loss was {:.6f}, acc was {:.6f} at epoch {}.'\n",
    "                    .format(current_valid, current_valid_acc, current_train_loss, current_train_acc, current_epoch+1))\n",
    "                if current_valid < best_valid:\n",
    "                    best_valid = current_valid\n",
    "                    best_valid_acc = current_valid_acc\n",
    "                    best_valid_epoch = current_epoch\n",
    "\n",
    "                    # save the model parameters\n",
    "                    modelsaver(network=network, path=save_path, epoch_identifier=None)\n",
    "                    log('Best valid loss was {:.6f} and acc {:.6f} at epoch {}.'.format(\n",
    "                          best_valid, best_valid_acc, best_valid_epoch+1))\n",
    "                elif best_valid_epoch + patience < current_epoch:\n",
    "                    log('Early stopping.')\n",
    "                    log('Best valid loss was {:.6f} and acc {:.6f} at epoch {}.'.format(\n",
    "                          best_valid, best_valid_acc, best_valid_epoch+1))\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            current_val_loss = val_loss / n_batch_val\n",
    "            current_val_acc = val_acc / n_batch_val\n",
    "            current_epoch = epoch\n",
    "            current_train_loss = train_loss / n_batch_train\n",
    "            current_train_acc = train_acc / n_batch_train\n",
    "            log('    Current valid loss was {:.6f}, acc was {:.6f}, train loss was {:.6f}, acc was {:.6f} at epoch {}.'\n",
    "                .format(current_val_loss, current_val_acc, current_train_loss, current_train_acc, current_epoch+1))\n",
    "\n",
    "            #log(save_model > 0, epoch % save_model == 0, epoch/save_model > 0)\n",
    "            if save_model > 0 and epoch % save_model == 0:\n",
    "                if epoch_identifier:\n",
    "                    modelsaver(network=network, path=save_path, epoch_identifier=epoch+1)\n",
    "                else:\n",
    "                    modelsaver(network=network, path=save_path, epoch_identifier=None)\n",
    "\n",
    "    if not earlystopping:\n",
    "        if save_model == -1:\n",
    "            modelsaver(network=network, path=save_path, epoch_identifier=None)\n",
    "\n",
    "    log(\"Total training time: %fs\" % (time.time() - start_time_begin))\n",
    "    return total_train_loss, total_val_loss, single_step_train_loss, single_step_val_loss\n",
    "\n",
    "\n",
    "def custompredictX(sess,\n",
    "                  network,\n",
    "                  output_provider,\n",
    "                  x,\n",
    "                  fragment_size=1000,\n",
    "                  output_length=1,\n",
    "                  y_op=None,\n",
    "                  out_kwag=None):\n",
    "    \"\"\"\n",
    "        Return the predict results of given non time-series network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sess : TensorFlow session\n",
    "            sess = tf.InteractiveSession()\n",
    "        network : a TensorLayer layer\n",
    "            the network will be trained\n",
    "        x : placeholder\n",
    "            the input\n",
    "        y_op : placeholder\n",
    "    \"\"\"\n",
    "\n",
    "    if y_op is None:\n",
    "        y_op = network.outputs\n",
    "\n",
    "    output_container = []\n",
    "    banum = 0\n",
    "\n",
    "    for X_out in output_provider.feed(out_kwag['inputs']):\n",
    "        #log(banum)\n",
    "        #banum += 1\n",
    "\n",
    "        feed_dict = {x: X_out,}\n",
    "        output = sess.run(y_op, feed_dict=feed_dict)\n",
    "        output_array = np.array(output[0]).reshape(-1, output_length)\n",
    "        output_container.append(output_array)\n",
    "\n",
    "    return np.vstack(output_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110acec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_TP(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the  number of true positive\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    assert (target.shape == prediction.shape)\n",
    "\n",
    "    target = 1 - np.clip(target, threshold, 0) / threshold\n",
    "    prediction = 1 - np.clip(prediction, threshold, 0) / threshold\n",
    "\n",
    "    tp_array = np.logical_and(target, prediction) * 1.0\n",
    "    tp = np.sum(tp_array)\n",
    "\n",
    "    return tp\n",
    "\n",
    "\n",
    "def get_FP(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the  number of false positive\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    assert (target.shape == prediction.shape)\n",
    "\n",
    "    target = np.clip(target, threshold, 0) / threshold\n",
    "    prediction = 1 - np.clip(prediction, threshold, 0) / threshold\n",
    "\n",
    "    fp_array = np.logical_and(target, prediction) * 1.0\n",
    "    fp = np.sum(fp_array)\n",
    "\n",
    "    return fp\n",
    "\n",
    "\n",
    "def get_FN(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the  number of false negtive\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    assert (target.shape == prediction.shape)\n",
    "\n",
    "    target = 1 - np.clip(target, threshold, 0) / threshold\n",
    "    prediction = np.clip(prediction, threshold, 0) / threshold\n",
    "\n",
    "    fn_array = np.logical_and(target, prediction) * 1.0\n",
    "    fn = np.sum(fn_array)\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def get_TN(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the  number of true negative\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    assert (target.shape == prediction.shape)\n",
    "\n",
    "    target = np.clip(target, threshold, 0) / threshold\n",
    "    prediction = np.clip(prediction, threshold, 0) / threshold\n",
    "\n",
    "    tn_array = np.logical_and(target, prediction) * 1.0\n",
    "    tn = np.sum(tn_array)\n",
    "\n",
    "    return tn\n",
    "\n",
    "\n",
    "def get_recall(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the recall rate\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    tp = get_TP(target, prediction, threshold)\n",
    "    fn = get_FN(target, prediction, threshold)\n",
    "    log('tp={0}'.format(tp))\n",
    "    log('fn={0}'.format(fn))\n",
    "    if tp + fn <= 0.0:\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def get_precision(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the  precision rate\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    tp = get_TP(target, prediction, threshold)\n",
    "    fp = get_FP(target, prediction, threshold)\n",
    "    log('tp={0}'.format(tp))\n",
    "    log('fp={0}'.format(fp))\n",
    "    if tp + fp <= 0.0:\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def get_F1(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the  F1 score\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    recall = get_recall(target, prediction, threshold)\n",
    "    log(recall)\n",
    "    precision = get_precision(target, prediction, threshold)\n",
    "    log(precision)\n",
    "    if precision == 0.0 or recall == 0.0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def get_accuracy(target, prediction, threshold):\n",
    "    '''\n",
    "    compute the accuracy rate\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    threshold: float\n",
    "    '''\n",
    "\n",
    "    tp = get_TP(target, prediction, threshold)\n",
    "    tn = get_TN(target, prediction, threshold)\n",
    "\n",
    "    accuracy = (tp + tn) / target.size\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_relative_error(target, prediction):\n",
    "    '''\n",
    "    compute the  relative_error\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    '''\n",
    "\n",
    "    assert (target.shape == prediction.shape)\n",
    "\n",
    "    return np.mean(np.nan_to_num(np.abs(target - prediction) / np.maximum(target, prediction)))\n",
    "\n",
    "\n",
    "def get_abs_error(target, prediction):\n",
    "    '''\n",
    "    compute the  absolute_error\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    '''\n",
    "\n",
    "    assert (target.shape == prediction.shape)\n",
    "\n",
    "    data = np.abs(target - prediction)\n",
    "    mean, std, min_v, max_v, quartile1, median, quartile2 = get_statistics(data)\n",
    "\n",
    "    return mean, std, min_v, max_v, quartile1, median, quartile2, data\n",
    "\n",
    "\n",
    "def get_nde(target, prediction):\n",
    "    '''\n",
    "    compute the  normalized disaggregation error\n",
    "\n",
    "    Parameters:\n",
    "    ----------------\n",
    "    target: the groud truth , np.array\n",
    "    prediction: the prediction, np.array\n",
    "    '''\n",
    "\n",
    "    return np.sum((target - prediction) ** 2) / np.sum((target ** 2))\n",
    "\n",
    "\n",
    "def get_sae(target, prediction, sample_second):\n",
    "    '''\n",
    "    compute the signal aggregate error\n",
    "    sae = |\\hat(r)-r|/r where r is the ground truth total energy;\n",
    "    \\hat(r) is the predicted total energy.\n",
    "    '''\n",
    "    r = np.sum(target * sample_second * 1.0 / 3600.0)\n",
    "    rhat = np.sum(prediction * sample_second * 1.0 / 3600.0)\n",
    "\n",
    "    sae = np.abs(r - rhat) / np.abs(r)\n",
    "\n",
    "    return sae\n",
    "\n",
    "def get_Epd(target, prediction, sample_second):\n",
    "    '''\n",
    "    Energy per day\n",
    "    - calculate energy of a day for both ground truth and prediction\n",
    "    - sum all the energies\n",
    "    - divide by the number of days\n",
    "    '''\n",
    "\n",
    "    day = int(24.0 * 3600 / sample_second)\n",
    "    gt_en_days = []\n",
    "    pred_en_days = []\n",
    "\n",
    "    for start in range(0, int(len(target)-day), int(day)):\n",
    "        gt_en_days.append(np.sum(target[start:start+day]*sample_second)/3600)\n",
    "        pred_en_days.append(np.sum(prediction[start:start+day]*sample_second)/3600)\n",
    "\n",
    "    Epd = np.sum(np.abs(np.array(gt_en_days)-np.array(pred_en_days)))/(len(target)/day)\n",
    "\n",
    "    return Epd\n",
    "\n",
    "\n",
    "def get_statistics(data):\n",
    "\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    min_v = np.sort(data)[0]\n",
    "    max_v = np.sort(data)[-1]\n",
    "\n",
    "    quartile1 = np.percentile(data, 25)\n",
    "    median = np.percentile(data, 50)\n",
    "    quartile2 = np.percentile(data, 75)\n",
    "\n",
    "    return mean, std, min_v, max_v, quartile1, median, quartile2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cc836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 03:48:32,050 [INFO ]  Arguments: \n",
      "2025-06-20 03:48:32,069 [INFO ]  <__main__.get_arguments object at 0x00000286CE587760>\n",
      "2025-06-20 03:48:32,074 [INFO ]  Training dataset: UKDALE/kettle/kettle_training_.csv\n",
      "2025-06-20 03:48:32,075 [INFO ]  kettle_validation_.csv\n",
      "2025-06-20 03:48:32,077 [INFO ]  Validation dataset: UKDALE/kettle/kettle_validation_.csv\n",
      "2025-06-20 03:48:32,558 [INFO ]  Standard training...\n",
      "2025-06-20 03:48:32,558 [INFO ]  ...creating a new model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 599)]             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 599, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 599, 30)        330       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 599, 30)        7230      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 599, 40)        7240      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 599, 50)        10050     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 599, 50)        12550     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 29950)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              30669824  \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,708,249\n",
      "Trainable params: 30,708,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 03:48:32,733 [INFO ]  All network parameters: \n",
      "2025-06-20 03:48:32,737 [INFO ]  ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'dense/kernel:0', 'dense/bias:0', 'output/kernel:0', 'output/bias:0']\n",
      "2025-06-20 03:48:32,740 [INFO ]  Trainable parameters:\n",
      "2025-06-20 03:48:32,742 [INFO ]  ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'dense/kernel:0', 'dense/bias:0', 'output/kernel:0', 'output/bias:0']\n",
      "2025-06-20 03:48:34,297 [INFO ]  TensorFlow Session starting...\n",
      "2025-06-20 03:48:35,087 [INFO ]  TensorBoard infos in ./tensorboard_test\n",
      "2025-06-20 03:48:35,292 [INFO ]  Start training the network ...\n",
      "2025-06-20 03:48:35,330 [INFO ]  ------------------------- Epoch 1 of 5 --------------------------\n",
      "2025-06-20 03:48:36,086 [INFO ]  Size of the dataset is 0.010 M rows.\n",
      "2025-06-20 03:48:36,086 [INFO ]  This size can fit the memory so it will load entirely\n",
      "2025-06-20 03:49:08,945 [INFO ]  loss_ep: 0.122456\n",
      "2025-06-20 03:49:08,989 [INFO ]  Epoch 1 of 5 took 33.659573s\n",
      "2025-06-20 03:49:08,989 [INFO ]  Validation...\n",
      "2025-06-20 03:49:11,149 [INFO ]     train loss/n_batch_train: 0.052341\n",
      "2025-06-20 03:49:11,151 [INFO ]     train loss: 0.523406, n_batch_train: 10\n",
      "2025-06-20 03:49:11,278 [INFO ]  Size of the dataset is 0.010 M rows.\n",
      "2025-06-20 03:49:11,280 [INFO ]  This size can fit the memory so it will load entirely\n",
      "2025-06-20 03:49:13,131 [INFO ]      val loss: 0.073869\n",
      "2025-06-20 03:49:13,141 [INFO ]  ------------------------- Epoch 2 of 5 --------------------------\n",
      "2025-06-20 03:49:18,246 [INFO ]  loss_ep: 0.024993\n",
      "2025-06-20 03:49:18,247 [INFO ]  Epoch 2 of 5 took 5.105611s\n",
      "2025-06-20 03:49:18,247 [INFO ]  Validation...\n",
      "2025-06-20 03:49:20,203 [INFO ]     train loss/n_batch_train: 0.013254\n",
      "2025-06-20 03:49:20,203 [INFO ]     train loss: 0.132538, n_batch_train: 10\n",
      "2025-06-20 03:49:22,087 [INFO ]      val loss: 0.031174\n",
      "2025-06-20 03:49:22,087 [INFO ]  Evaluate earlystopping parameters...\n",
      "2025-06-20 03:49:22,101 [INFO ]      Current valid loss was 0.031174, acc was 0.000000, train loss was 0.013254, acc was 0.000000 at epoch 2.\n",
      "2025-06-20 03:49:23,833 [DEBUG]  Creating converter from 5 to 3\n",
      "2025-06-20 03:49:28,863 [INFO ]  Best valid loss was 0.031174 and acc 0.000000 at epoch 2.\n",
      "2025-06-20 03:49:28,864 [INFO ]  ------------------------- Epoch 3 of 5 --------------------------\n",
      "2025-06-20 03:49:34,355 [INFO ]  loss_ep: 0.015406\n",
      "2025-06-20 03:49:34,358 [INFO ]  Epoch 3 of 5 took 5.494418s\n",
      "2025-06-20 03:49:34,360 [INFO ]  Validation...\n",
      "2025-06-20 03:49:36,213 [INFO ]     train loss/n_batch_train: 0.013618\n",
      "2025-06-20 03:49:36,213 [INFO ]     train loss: 0.136178, n_batch_train: 10\n",
      "2025-06-20 03:49:38,170 [INFO ]      val loss: 0.031098\n",
      "2025-06-20 03:49:38,172 [INFO ]  Evaluate earlystopping parameters...\n",
      "2025-06-20 03:49:38,173 [INFO ]      Current valid loss was 0.031098, acc was 0.000000, train loss was 0.013618, acc was 0.000000 at epoch 3.\n",
      "2025-06-20 03:49:42,531 [INFO ]  Best valid loss was 0.031098 and acc 0.000000 at epoch 3.\n",
      "2025-06-20 03:49:42,531 [INFO ]  ------------------------- Epoch 4 of 5 --------------------------\n",
      "2025-06-20 03:49:47,877 [INFO ]  loss_ep: 0.013772\n",
      "2025-06-20 03:49:47,878 [INFO ]  Epoch 4 of 5 took 5.347063s\n",
      "2025-06-20 03:49:47,901 [INFO ]  Validation...\n",
      "2025-06-20 03:49:49,716 [INFO ]     train loss/n_batch_train: 0.015433\n",
      "2025-06-20 03:49:49,726 [INFO ]     train loss: 0.154333, n_batch_train: 10\n",
      "2025-06-20 03:49:51,540 [INFO ]      val loss: 0.031301\n",
      "2025-06-20 03:49:51,540 [INFO ]  Evaluate earlystopping parameters...\n",
      "2025-06-20 03:49:51,583 [INFO ]      Current valid loss was 0.031301, acc was 0.000000, train loss was 0.015433, acc was 0.000000 at epoch 4.\n",
      "2025-06-20 03:49:51,583 [INFO ]  ------------------------- Epoch 5 of 5 --------------------------\n",
      "2025-06-20 03:49:56,681 [INFO ]  loss_ep: 0.013905\n",
      "2025-06-20 03:49:56,681 [INFO ]  Epoch 5 of 5 took 5.098336s\n",
      "2025-06-20 03:49:56,685 [INFO ]  Validation...\n",
      "2025-06-20 03:49:58,526 [INFO ]     train loss/n_batch_train: 0.014389\n",
      "2025-06-20 03:49:58,526 [INFO ]     train loss: 0.143891, n_batch_train: 10\n",
      "2025-06-20 03:50:00,332 [INFO ]      val loss: 0.030672\n",
      "2025-06-20 03:50:00,332 [INFO ]  Evaluate earlystopping parameters...\n",
      "2025-06-20 03:50:00,332 [INFO ]      Current valid loss was 0.030672, acc was 0.000000, train loss was 0.014389, acc was 0.000000 at epoch 5.\n",
      "2025-06-20 03:50:05,654 [INFO ]  Best valid loss was 0.030672 and acc 0.000000 at epoch 5.\n",
      "2025-06-20 03:50:05,654 [INFO ]  Total training time: 90.323668s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "def remove_space(string):\n",
    "    return string.replace(\" \",\"\")\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network\\\n",
    "                                     for energy disaggregation - \\\n",
    "                                     network input = mains window; \\\n",
    "                                     network target = the states of \\\n",
    "                                     the target appliance.')\n",
    "    parser.add_argument('--appliance_name',\n",
    "                        type=remove_space,\n",
    "                        default='kettle',\n",
    "                        help='the name of target appliance')\n",
    "    parser.add_argument('--datadir',\n",
    "                        type=str,\n",
    "                        default='/media/michele/Dati/myREFIT/',\n",
    "                        help='this is the directory of the training samples')\n",
    "    parser.add_argument('--pretrainedmodel_dir',\n",
    "                        type=str,\n",
    "                        default='./pretrained_model',\n",
    "                        help='this is the directory of the pre-trained models')\n",
    "    parser.add_argument('--save_dir',\n",
    "                        type=str,\n",
    "                        default='./models',\n",
    "                        help='this is the directory to save the trained models')\n",
    "    parser.add_argument('--batchsize',\n",
    "                        type=int,\n",
    "                        default=1000,\n",
    "                        help='The batch size of training examples')\n",
    "    parser.add_argument('--n_epoch',\n",
    "                        type=int,\n",
    "                        default=5,\n",
    "                        help='The number of epochs.')\n",
    "    parser.add_argument('--save_model',\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help='Save the learnt model:\\\n",
    "                        0 -- not to save the learnt model parameters;\\\n",
    "                        n (n>0) -- to save the model params every n steps;\\\n",
    "                        -1 -- only save the learnt model params\\\n",
    "                        at the end of training.')\n",
    "    parser.add_argument('--dense_layers',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=':\\\n",
    "                                1 -- One dense layers (default Seq2point);\\\n",
    "                                2 -- Two dense layers;\\\n",
    "                                3 -- Three dense layers.')\n",
    "    parser.add_argument(\"--transfer_model\", type=str2bool,\n",
    "                        default=False,\n",
    "                        help=\"True: using entire pre-trained model.\\\n",
    "                             False: retrain the entire pre-trained model;\\\n",
    "                             This will override the 'transfer_cnn' and 'cnn' parameters;\\\n",
    "                             The appliance_name parameter will use to retrieve \\\n",
    "                             the entire pre-trained model of that appliance.\")\n",
    "    parser.add_argument(\"--transfer_cnn\", type=str2bool,\n",
    "                        default=False,\n",
    "                        help=\"True: using a pre-trained CNN\\\n",
    "                              False: not using a pre-trained CNN.\")\n",
    "    parser.add_argument('--cnn',\n",
    "                        type=str,\n",
    "                        default='kettle',\n",
    "                        help='The CNN trained by which appliance to load (pretrained model).')\n",
    "    parser.add_argument('--gpus',\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help='Number of GPUs to use:\\\n",
    "                            n -- number of GPUs the system should use;\\\n",
    "                            -1 -- do not use any GPU.')\n",
    "    parser.add_argument('--crop_dataset',\n",
    "                        type=int,\n",
    "                        default=10000,\n",
    "                        help='for debugging porpose should be helpful to crop the training dataset size')\n",
    "    parser.add_argument('--ram',\n",
    "                        type=int,\n",
    "                        default=5*10**5,\n",
    "                        help='Maximum number of rows of csv dataset can handle without loading in chunks')\n",
    "    return parser.parse_args()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class get_arguments:\n",
    "    def __init__(self):\n",
    "        self.data_dir = 'UKDALE/'\n",
    "        self.appliance_name = 'kettle'\n",
    "        self.aggregate_mean = AGG_MEAN\n",
    "        self.aggregate_std = AGG_STD\n",
    "        self.save_path = SAVE_PATH  # Added to fix AttributeError\n",
    "        self.save_dir = 'transfer_learned_models/'  # Added to fix AttributeError\n",
    "        self.pretrainedmodel_dir = 'saved_models/kettle_best_model'\n",
    "        self.batchsize = 1000\n",
    "        self.n_epoch = 5\n",
    "        self.save_model = -1\n",
    "        self.dense_layers = 1\n",
    "        self.transfer_model = False\n",
    "        self.transfer_cnn = False\n",
    "        self.cnn = 'kettle'\n",
    "        self.gpus = 1\n",
    "        self.crop_dataset = 10000\n",
    "        self.ram = 5*10**5\n",
    "\n",
    "\n",
    "\n",
    "args = get_arguments()\n",
    "log('Arguments: ')\n",
    "log(args)\n",
    "\n",
    "# some constant parameters\n",
    "CHUNK_SIZE = 5*10**6\n",
    "\n",
    "# Reset the default graph and start the session for training a network\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "# the appliance to train on\n",
    "appliance_name = args.appliance_name\n",
    "\n",
    "# path for training data\n",
    "training_path = args.data_dir + appliance_name + '/' + appliance_name + '_training_' + '.csv'\n",
    "log('Training dataset: ' + training_path)\n",
    "\n",
    "# Looking for the validation set\n",
    "for filename in os.listdir(args.data_dir + appliance_name):\n",
    "    if \"validation\" in filename:\n",
    "        val_filename = filename\n",
    "        log(val_filename)\n",
    "\n",
    "# path for validation data\n",
    "validation_path = args.data_dir + appliance_name + '/' + val_filename\n",
    "log('Validation dataset: ' + validation_path)\n",
    "\n",
    "\n",
    "# offset parameter from window length\n",
    "offset = int(0.5*(params_appliance[args.appliance_name]['windowlength']-1.0))\n",
    "\n",
    "# Defining object for training set loading and windowing provider (DataProvider.py)\n",
    "tra_provider = ChunkDoubleSourceSlider2(filename=training_path,\n",
    "                                        batchsize=args.batchsize,\n",
    "                                        chunksize = CHUNK_SIZE,\n",
    "                                        crop=args.crop_dataset,\n",
    "                                        shuffle=True,\n",
    "                                        offset=offset,\n",
    "                                        header=0,\n",
    "                                        ram_threshold=args.ram)\n",
    "\n",
    "# Defining object for validation set loading and windowing provider (DataProvider.py)\n",
    "val_provider = ChunkDoubleSourceSlider2(filename=validation_path,\n",
    "                                        batchsize=args.batchsize,\n",
    "                                        chunksize=CHUNK_SIZE,\n",
    "                                        crop=args.crop_dataset,\n",
    "                                        shuffle=False,\n",
    "                                        offset=offset,\n",
    "                                        header=0,\n",
    "                                        ram_threshold=args.ram)\n",
    "\n",
    "# TensorFlow placeholders\n",
    "x = tf.compat.v1.placeholder(tf.float32,\n",
    "                   shape=[None, params_appliance[args.appliance_name]['windowlength']],\n",
    "                   name='x')\n",
    "\n",
    "y_ = tf.compat.v1.placeholder(tf.float32,\n",
    "                    shape=[None, 1],\n",
    "                    name='y_')\n",
    "\n",
    "# -------------------------------- Keras Network - from model.py -----------------------------------------\n",
    "inp = Input(tensor=x)\n",
    "model, cnn_check_weights = get_model(args.appliance_name,\n",
    "                                     inp,\n",
    "                                     params_appliance[args.appliance_name]['windowlength'],\n",
    "                                     transfer_dense=args.transfer_model,\n",
    "                                     transfer_cnn=args.transfer_cnn,\n",
    "                                     cnn=args.cnn,\n",
    "                                     pretrainedmodel_dir=args.pretrainedmodel_dir)\n",
    "y = model.outputs\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(y, y_), 1))\n",
    "\n",
    "# model's weights to be trained\n",
    "train_params = tf.compat.v1.trainable_variables()\n",
    "log(\"All network parameters: \")\n",
    "log([v.name for v in train_params])\n",
    "# if transfer learning is selected, just the dense layer will be trained\n",
    "if not args.transfer_model and args.transfer_cnn:\n",
    "    parameters = 10\n",
    "else:\n",
    "    parameters = 0\n",
    "log(\"Trainable parameters:\")\n",
    "log([v.name for v in train_params[parameters:]])\n",
    "\n",
    "# Training hyper parameters\n",
    "train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                            beta1=0.9,\n",
    "                                            beta2=0.999,\n",
    "                                            epsilon=1e-08,\n",
    "                                            use_locking=False).minimize(cost,\n",
    "                                                                        var_list=train_params[parameters:]\n",
    "                                                                        )\n",
    "\n",
    "# Initialize all variables in the graph (must be after model is built)\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "\n",
    "log('TensorFlow Session starting...')\n",
    "\n",
    "# TensorBoard summary (graph)\n",
    "tf.compat.v1.summary.scalar('cost', cost)\n",
    "merged_summary = tf.compat.v1.summary.merge_all()\n",
    "writer = tf.compat.v1.summary.FileWriter('./tensorboard_test')\n",
    "writer.add_graph(sess.graph)\n",
    "log('TensorBoard infos in ./tensorboard_test')\n",
    "\n",
    "# Save path depending on the training behaviour\n",
    "if not args.transfer_model and args.transfer_cnn:\n",
    "    save_path = args.save_dir+'/cnn_s2p_' + appliance_name + '_transf_' + args.cnn + '_pointnet_model'\n",
    "else:\n",
    "    save_path = args.save_dir+'/cnn_s2p_' + appliance_name + '_pointnet_model'\n",
    "    \n",
    "if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "# Replace custom training function with standard Keras training loop\n",
    "# You may need to adapt this to your data provider's API\n",
    "\n",
    "train_loss, val_loss, step_train_loss, step_val_loss = customfit(sess=sess,\n",
    "                                                                    network=model,\n",
    "                                                                    cost=cost,\n",
    "                                                                    train_op=train_op,\n",
    "                                                                    tra_provider=tra_provider,\n",
    "                                                                    x=x,\n",
    "                                                                    y_=y_,\n",
    "                                                                    acc=None,\n",
    "                                                                    n_epoch=args.n_epoch,\n",
    "                                                                    print_freq=1,\n",
    "                                                                    val_provider=val_provider,\n",
    "                                                                    save_model=args.save_model,\n",
    "                                                                    save_path=save_path,\n",
    "                                                                    epoch_identifier=None,\n",
    "                                                                    earlystopping=True,\n",
    "                                                                    min_epoch=1,\n",
    "                                                                    patience=1)\n",
    "\n",
    "# Following are training info\n",
    "\"\"\"\n",
    "log('train loss: ' + str(train_loss))\n",
    "log('val loss: ' + str(val_loss))\n",
    "infos = pd.DataFrame(data={'train_loss': step_train_loss,\n",
    "                           #'val_loss': step_val_loss\n",
    "                           })\n",
    "infos.to_csv('./training_infos-{:}-{:}-{:}.csv'.format(appliance_name, args.transfer, args.cnn))\n",
    "log('training infos in .csv file')\n",
    "\"\"\"\n",
    "\n",
    "# This check that the CNN is the same of the beginning\n",
    "if not args.transfer_model and args.transfer_cnn:\n",
    "    log('Transfer learning check ...')\n",
    "    session = K.get_session()\n",
    "    for v in tf.trainable_variables():\n",
    "        if v.name == 'conv2d_1/kernel:0':\n",
    "            value = session.run(v)\n",
    "            vl = np.array(value).flatten()\n",
    "            c1 = np.array(cnn_check_weights).flatten()\n",
    "            if False in vl == c1:\n",
    "                log('Transfer check --- ERROR ---')\n",
    "            else:\n",
    "                log('Transfer check --- OK ---')\n",
    "\n",
    "\n",
    "sess.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
