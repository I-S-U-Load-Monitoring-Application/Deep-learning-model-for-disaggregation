{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b63e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_appliance = {\n",
    "    'kettle': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 2000,\n",
    "        'max_on_power': 3998,\n",
    "        'mean': 700,\n",
    "        'std': 1000,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [10, 8],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'microwave': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 200,\n",
    "        'max_on_power': 3969,\n",
    "        'mean': 500,\n",
    "        'std': 800,\n",
    "        's2s_length': 128,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [13, 15],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    'fridge': {\n",
    "        'windowlength': 599,\n",
    "        'on_power_threshold': 50,\n",
    "        'max_on_power': 3323,\n",
    "        'mean': 200,\n",
    "        'std': 400,\n",
    "        's2s_length': 512,\n",
    "        'houses': [1, 2],\n",
    "        'channels': [12, 14],\n",
    "        'train_build': [1],\n",
    "        'test_build': 2,\n",
    "    },\n",
    "    \n",
    "   # 'television': { \n",
    "   #     'windowlength': 599,  # You may keep or adjust this\n",
    "   #     'on_power_threshold': 10,\n",
    "   #     'max_on_power': ?,     # To be calculated from meter data\n",
    "   #     'mean': ?,             # Requires usage data\n",
    "   #     'std': ?,              # Requires usage data\n",
    "   #     's2s_length': ?,       # Typical sequence length for s2s models\n",
    "   #     'houses': [1],\n",
    "   #     'channels': [7],\n",
    "   #     'train_build': [1],\n",
    "   #     'test_build': []\n",
    "   # }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "DATA_DIRECTORY = 'dataset/ukdale/'\n",
    "SAVE_PATH = 'ukdale_seq2point/microwave/'\n",
    "AGG_MEAN = 522\n",
    "AGG_STD = 814\n",
    "APPLIANCE_NAME = 'microwave'\n",
    "\n",
    "\n",
    "\n",
    "mains_data = {\n",
    "    \"mean\": 522,\n",
    "    \"std\":  814        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae31f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_dataframe(directory, building, channel, col_names=['time', 'data'], nrows=None):\n",
    "    df = pd.read_table(directory + 'house_' + str(building) + '/' + 'channel_' +\n",
    "                       str(channel) + '.dat',\n",
    "                       sep=\"\\s+\",\n",
    "                       nrows=nrows,\n",
    "                       usecols=[0, 1],\n",
    "                       names=col_names,\n",
    "                       dtype={'time': str},\n",
    "                       )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "training_directory=\"refit/microwave/microwave_training_.csv\"\n",
    "validation_directory=\"refit/microwave/microwave_validation_H5.csv\"\n",
    "save_model_dir=\"saved_models/ukdale_models/\"\n",
    "\n",
    "\n",
    "\n",
    "class get_arguments:\n",
    "    def __init__(self):\n",
    "        self.data_dir = DATA_DIRECTORY\n",
    "        self.appliance_name = APPLIANCE_NAME\n",
    "        self.aggregate_mean = 522\n",
    "        self.aggregate_std = 814\n",
    "        self.save_path = SAVE_PATH\n",
    "        self.batch_size = 1000\n",
    "        self.crop = 10000\n",
    "        self.prunning_algorithm = \"threshold\"\n",
    "        self.network_type = \"seq2point\"\n",
    "        self.epochs = 20\n",
    "        self.input_window_length = 599\n",
    "        self.validation_frequency = 1\n",
    "\n",
    "\n",
    "args = get_arguments()\n",
    "appliance_name = args.appliance_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ebc6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:86: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if training_building_percent is not 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dataset/ukdale/house_1/channel_13.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:25: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:38: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:53: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample(str(sample_seconds) + 'S').mean().fillna(method='backfill', limit=1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:52: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_align = mains_df.join(app_df, how='outer'). \\\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train = pd.concat([train, df_align], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dataset/ukdale/house_2/channel_15.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:25: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:38: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:53: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample(str(sample_seconds) + 'S').mean().fillna(method='backfill', limit=1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6124\\840787682.py:52: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_align = mains_df.join(app_df, how='outer'). \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Size of test set is 1.2936 M rows.\n",
      "    Size of total training set is 0.7261 M rows.\n",
      "    Size of total validation set is 0.1085 M rows.\n",
      "\n",
      "Please find files in: ukdale_seq2point/microwave/\n",
      "Total elapsed time: 22.56 min.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    sample_seconds = 8\n",
    "    training_building_percent = 95\n",
    "    validation_percent = 13\n",
    "    nrows = None\n",
    "    debug = False\n",
    "\n",
    "    train = pd.DataFrame(columns=['aggregate', appliance_name])\n",
    "\n",
    "    for h in params_appliance[appliance_name]['houses']:\n",
    "        print('    ' + args.data_dir + 'house_' + str(h) + '/'\n",
    "              + 'channel_' +\n",
    "              str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "              '.dat')\n",
    "\n",
    "        mains_df = load_dataframe(args.data_dir, h, 1)\n",
    "        app_df = load_dataframe(args.data_dir,\n",
    "                                h,\n",
    "                                params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "                                col_names=['time', appliance_name]\n",
    "                                )\n",
    "\n",
    "        mains_df['time'] = pd.to_datetime(mains_df['time'], unit='s')\n",
    "        mains_df.set_index('time', inplace=True)\n",
    "        mains_df.columns = ['aggregate']\n",
    "        #resample = mains_df.resample(str(sample_seconds) + 'S').mean()\n",
    "        mains_df.reset_index(inplace=True)\n",
    "\n",
    "        if debug:\n",
    "            print(\"    mains_df:\")\n",
    "            print(mains_df.head())\n",
    "            plt.plot(mains_df['time'], mains_df['aggregate'])\n",
    "            plt.show()\n",
    "\n",
    "        # Appliance\n",
    "        app_df['time'] = pd.to_datetime(app_df['time'], unit='s')\n",
    "\n",
    "        if debug:\n",
    "            print(\"app_df:\")\n",
    "            print(app_df.head())\n",
    "            plt.plot(app_df['time'], app_df[appliance_name])\n",
    "            plt.show()\n",
    "\n",
    "        # the timestamps of mains and appliance are not the same, we need to align them\n",
    "        # 1. join the aggragte and appliance dataframes;\n",
    "        # 2. interpolate the missing values;\n",
    "        mains_df.set_index('time', inplace=True)\n",
    "        app_df.set_index('time', inplace=True)\n",
    "\n",
    "        df_align = mains_df.join(app_df, how='outer'). \\\n",
    "            resample(str(sample_seconds) + 'S').mean().fillna(method='backfill', limit=1)\n",
    "        df_align = df_align.dropna()\n",
    "\n",
    "        df_align.reset_index(inplace=True)\n",
    "\n",
    "        del mains_df, app_df, df_align['time']\n",
    "\n",
    "        if debug:\n",
    "            # plot the dtaset\n",
    "            print(\"df_align:\")\n",
    "            print(df_align.head())\n",
    "            plt.plot(df_align['aggregate'].values)\n",
    "            plt.plot(df_align[appliance_name].values)\n",
    "            plt.show()\n",
    "\n",
    "        # Normilization ----------------------------------------------------------------------------------------------\n",
    "        mean = params_appliance[appliance_name]['mean']\n",
    "        std = params_appliance[appliance_name]['std']\n",
    "\n",
    "        df_align['aggregate'] = (df_align['aggregate'] - args.aggregate_mean) / args.aggregate_std\n",
    "        df_align[appliance_name] = (df_align[appliance_name] - mean) / std\n",
    "\n",
    "        if h == params_appliance[appliance_name]['test_build']:\n",
    "            # Test CSV\n",
    "            df_align.to_csv(args.save_path + appliance_name + '_test_.csv', mode='a', index=False, header=False)\n",
    "            print(\"    Size of test set is {:.4f} M rows.\".format(len(df_align) / 10 ** 6))\n",
    "            continue\n",
    "\n",
    "        \n",
    "        train = pd.concat([train, df_align], ignore_index=True)\n",
    "        del df_align\n",
    "\n",
    "    # Crop dataset\n",
    "    if training_building_percent is not 0:\n",
    "        train.drop(train.index[-int((len(train)/100)*training_building_percent):], inplace=True)\n",
    "\n",
    "\n",
    "    # Validation CSV\n",
    "    val_len = int((len(train)/100)*validation_percent)\n",
    "    val = train.tail(val_len)\n",
    "    val.reset_index(drop=True, inplace=True)\n",
    "    train.drop(train.index[-val_len:], inplace=True)\n",
    "    # Validation CSV\n",
    "    val.to_csv(args.save_path + appliance_name + '_validation_' + '.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    # Training CSV\n",
    "    train.to_csv(args.save_path + appliance_name + '_training_.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    print(\"    Size of total training set is {:.4f} M rows.\".format(len(train) / 10 ** 6))\n",
    "    print(\"    Size of total validation set is {:.4f} M rows.\".format(len(val) / 10 ** 6))\n",
    "    del train, val\n",
    "\n",
    "\n",
    "    print(\"\\nPlease find files in: \" + args.save_path)\n",
    "    print(\"Total elapsed time: {:.2f} min.\".format((time.time() - start_time) / 60))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604dc0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting creating testset...\n",
      "dataset/ukdale/house_1/channel_13.dat\n",
      "         time  microwave\n",
      "0  1355523693          1\n",
      "1  1355523699          1\n",
      "2  1355523705          1\n",
      "3  1355523711          1\n",
      "4  1355523717          1\n",
      "         time  microwave\n",
      "0  1352500095        599\n",
      "1  1352500101        582\n",
      "2  1352500107        600\n",
      "3  1352500113        586\n",
      "4  1352500120        596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:57: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:58: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  microwave\n",
      "0 1970-01-16 15:41:40.095        599\n",
      "1 1970-01-16 15:41:40.101        582\n",
      "2 1970-01-16 15:41:40.107        600\n",
      "3 1970-01-16 15:41:40.113        586\n",
      "4 1970-01-16 15:41:40.120        596\n",
      "                     time  microwave\n",
      "0 1970-01-16 16:32:03.693          1\n",
      "1 1970-01-16 16:32:03.699          1\n",
      "2 1970-01-16 16:32:03.705          1\n",
      "3 1970-01-16 16:32:03.711          1\n",
      "4 1970-01-16 16:32:03.717          1\n",
      "   aggregate  microwave\n",
      "0        599          1\n",
      "1        582          1\n",
      "2        600          1\n",
      "3        586          1\n",
      "4        596          1\n",
      "                     aggregate  microwave\n",
      "1970-01-01 00:00:00      590.5        1.0\n",
      "1970-01-01 00:00:08      600.0        1.0\n",
      "1970-01-01 00:00:16      586.0        1.0\n",
      "1970-01-01 00:00:24      588.5        1.0\n",
      "1970-01-01 00:00:32      597.0        1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:72: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:74: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample = df.resample('8S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set is 0.075 M rows (House 1).\n",
      "dataset/ukdale/house_2/channel_15.dat\n",
      "         time  microwave\n",
      "0  1369085319          0\n",
      "1  1369085325          0\n",
      "2  1369085331          0\n",
      "3  1369085337          0\n",
      "4  1369085344          0\n",
      "         time  microwave\n",
      "0  1361117854        340\n",
      "1  1361117860        341\n",
      "2  1361117866        347\n",
      "3  1361117872        350\n",
      "4  1361117878        342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:57: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:58: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time  microwave\n",
      "0 1970-01-16 18:05:17.854        340\n",
      "1 1970-01-16 18:05:17.860        341\n",
      "2 1970-01-16 18:05:17.866        347\n",
      "3 1970-01-16 18:05:17.872        350\n",
      "4 1970-01-16 18:05:17.878        342\n",
      "                     time  microwave\n",
      "0 1970-01-16 20:18:05.319          0\n",
      "1 1970-01-16 20:18:05.325          0\n",
      "2 1970-01-16 20:18:05.331          0\n",
      "3 1970-01-16 20:18:05.337          0\n",
      "4 1970-01-16 20:18:05.344          0\n",
      "   aggregate  microwave\n",
      "0        340          0\n",
      "1        341          0\n",
      "2        347          0\n",
      "3        350          0\n",
      "4        342          0\n",
      "                     aggregate  microwave\n",
      "1970-01-01 00:00:00      340.5        0.0\n",
      "1970-01-01 00:00:08      347.0        0.0\n",
      "1970-01-01 00:00:16      350.0        0.0\n",
      "1970-01-01 00:00:24      341.5        0.0\n",
      "1970-01-01 00:00:32      343.0        0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:72: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6448\\500205128.py:74: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  resample = df.resample('8S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set is 0.075 M rows (House 2).\n",
      "\n",
      "Normalization parameters: \n",
      "Mean and standard deviation values USED for AGGREGATE are:\n",
      "    Mean = 814, STD = 522\n",
      "Mean and standard deviation values USED for microwave are:\n",
      "    Mean = 500, STD = 800\n",
      "\n",
      "Please find files in: ukdale_seq2point/microwave/\n",
      "\n",
      "Total elapsed time: 0 min\n"
     ]
    }
   ],
   "source": [
    "nrows = 10**5\n",
    "path = DATA_DIRECTORY \n",
    "save_path = SAVE_PATH\n",
    "aggregate_std = AGG_MEAN \n",
    "aggregate_mean = AGG_STD\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def load(path, building, appliance, channel, nrows=None):\n",
    "    # load csv\n",
    "    file_name = path + 'house_' + str(building) + '/' + 'channel_' + str(channel) + '.dat'\n",
    "    single_csv = pd.read_csv(file_name,\n",
    "                             sep=' ',\n",
    "                             #header=0,\n",
    "                             names=['time', appliance],\n",
    "                             dtype={'time': str, \"appliance\": int},\n",
    "                             #parse_dates=['time'],\n",
    "                             #date_parser=pd.to_datetime,\n",
    "                             nrows=nrows,\n",
    "                             usecols=[0, 1],\n",
    "                             engine='python'\n",
    "                             )\n",
    "    return single_csv\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting creating testset...\")\n",
    "\n",
    "for h in params_appliance[appliance_name]['houses']:\n",
    "\n",
    "    print(path + 'house_' + str(h) + '/'\n",
    "          + 'channel_' +\n",
    "          str(params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)]) +\n",
    "          '.dat')\n",
    "\n",
    "    agg_df = load(path,\n",
    "                  h,\n",
    "                  appliance_name,\n",
    "                  1,\n",
    "                  nrows=nrows,\n",
    "                  )\n",
    "\n",
    "    df = load(path,\n",
    "              h,\n",
    "              appliance_name,\n",
    "              params_appliance[appliance_name]['channels'][params_appliance[appliance_name]['houses'].index(h)],\n",
    "              nrows=nrows,\n",
    "              )\n",
    "\n",
    "    #for i in range(100):\n",
    "    #    print(int(df['time'][i]) - int(agg_df['time'][i]))\n",
    "\n",
    "    # Time conversion\n",
    "    print(df.head())\n",
    "    print(agg_df.head())\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    agg_df['time'] = pd.to_datetime(agg_df['time'], unit='ms')\n",
    "    print(agg_df.head())\n",
    "    print(df.head())\n",
    "\n",
    "    df['aggregate'] = agg_df[appliance_name]\n",
    "    cols = df.columns.tolist()\n",
    "    del cols[0]\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    # Re-sampling\n",
    "    ind = pd.date_range(0,  periods=df.shape[0], freq='6S')\n",
    "    df.set_index(ind, inplace=True, drop=True)\n",
    "    resample = df.resample('8S')\n",
    "    df = resample.mean()\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    # Normalization\n",
    "    df['aggregate'] = (df['aggregate'] - aggregate_mean) / aggregate_std\n",
    "    df[appliance_name] = \\\n",
    "        (df[appliance_name] - params_appliance[appliance_name]['mean']) / params_appliance[appliance_name]['std']\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(save_path + appliance_name + '_test_' + 'uk-dale_' + 'H' + str(h) + '.csv', index=False)\n",
    "\n",
    "    print(\"Size of test set is {:.3f} M rows (House {:d}).\"\n",
    "          .format(df.shape[0] / 10 ** 6, h))\n",
    "\n",
    "    del df\n",
    "\n",
    "\n",
    "print(\"\\nNormalization parameters: \")\n",
    "print(\"Mean and standard deviation values USED for AGGREGATE are:\")\n",
    "print(\"    Mean = {:d}, STD = {:d}\".format(aggregate_mean, aggregate_std))\n",
    "\n",
    "print('Mean and standard deviation values USED for ' + appliance_name + ' are:')\n",
    "print(\"    Mean = {:d}, STD = {:d}\"\n",
    "      .format(params_appliance[appliance_name]['mean'], params_appliance[appliance_name]['std']))\n",
    "\n",
    "print(\"\\nPlease find files in: \" + save_path)\n",
    "tot = int(int(time.time() - start_time) / 60)\n",
    "print(\"\\nTotal elapsed time: \" + str(tot) + ' min')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b9788",
   "metadata": {},
   "source": [
    "Data Feeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5447e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# batch_size: the number of rows fed into the network at once.\n",
    "# crop: the number of rows in the data set to be used in total.\n",
    "# chunk_size: the number of lines to read from the file at once.\n",
    "\n",
    "class TrainSlidingWindowGenerator():\n",
    "\n",
    "    \"\"\"Yields features and targets for training a ConvNet.\n",
    "\n",
    "    Parameters:\n",
    "    __file_name (string): The path where the training dataset is located.\n",
    "    __batch_size (int): The size of each batch from the dataset to be processed.\n",
    "    __chunk_size (int): The size of each chunk of data to be processed.\n",
    "    __shuffle (bool): Whether the dataset should be shuffled before being returned.\n",
    "    __offset (int):\n",
    "    __crop (int): The number of rows of the dataset to return.\n",
    "    __skip_rows (int): The number of rows of a dataset to skip before reading data.\n",
    "    __ram_threshold (int): The maximum amount of RAM to utilise at a time.\n",
    "    total_size (int): The number of rows read from the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                file_name, \n",
    "                chunk_size, \n",
    "                shuffle, \n",
    "                offset, \n",
    "                batch_size=1000, \n",
    "                crop=100000, \n",
    "                skip_rows=0, \n",
    "                ram_threshold=5 * 10 ** 5):\n",
    "        self.__file_name = file_name\n",
    "        self.__batch_size = batch_size\n",
    "        self.__chunk_size = 10 ** 8\n",
    "        self.__shuffle = shuffle\n",
    "        self.__offset = offset\n",
    "        self.__crop = crop\n",
    "        self.__skip_rows = skip_rows\n",
    "        self.__ram_threshold = ram_threshold\n",
    "        self.__total_size = 0\n",
    "        self.__total_num_samples = crop\n",
    "\n",
    "    @property\n",
    "    def total_num_samples(self):\n",
    "        return self.__total_num_samples\n",
    "    \n",
    "    @total_num_samples.setter\n",
    "    def total_num_samples(self, value):\n",
    "        self.__total_num_samples = value\n",
    "\n",
    "    def check_if_chunking(self):\n",
    "\n",
    "        \"\"\"Count the number of rows in the dataset and determine whether this is larger than the chunking \n",
    "        threshold or not. \"\"\"\n",
    "\n",
    "        # Loads the file and counts the number of rows it contains.\n",
    "        print(\"Importing training file...\")\n",
    "        chunks = pd.read_csv(self.__file_name, \n",
    "                            header=0, \n",
    "                            nrows=self.__crop, \n",
    "                            skiprows=self.__skip_rows, skip_blank_lines=True)\n",
    "        print(\"Counting number of rows...\")\n",
    "        self.__total_size = len(chunks)\n",
    "        del chunks\n",
    "        print(\"Done.\")\n",
    "\n",
    "        print(\"The dataset contains \", self.__total_size, \" rows\")\n",
    "\n",
    "        # Display a warning if there are too many rows to fit in the designated amount RAM.\n",
    "        if (self.__total_size > self.__ram_threshold):\n",
    "            print(\"There is too much data to load into memory, so it will be loaded in chunks. Please note that this may result in decreased training times.\")\n",
    "    \n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\"Yields pairs of features and targets that will be used directly by a neural network for training.\n",
    "\n",
    "        Yields:\n",
    "        input_data (numpy.array): A 1D array of size batch_size containing features of a single input. \n",
    "        output_data (numpy.array): A 1D array of size batch_size containing the target values corresponding to \n",
    "        each feature set.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__total_size == 0:\n",
    "            self.check_if_chunking()\n",
    "\n",
    "        # If the data can be loaded in one go, don't skip any rows.\n",
    "        if (self.__total_size <= self.__ram_threshold):\n",
    "\n",
    "            # Returns an array of the content from the CSV file.\n",
    "            data_array = np.array(pd.read_csv(self.__file_name, nrows=self.__crop, skiprows=self.__skip_rows, header=0, skip_blank_lines=True))\n",
    "            inputs = data_array[:, 0]\n",
    "            outputs = data_array[:, 1]\n",
    "\n",
    "            maximum_batch_size = inputs.size - 2 * self.__offset\n",
    "            self.total_num_samples = maximum_batch_size\n",
    "            if self.__batch_size < 0:\n",
    "                self.__batch_size = maximum_batch_size\n",
    "\n",
    "            indicies = np.arange(maximum_batch_size)\n",
    "            if self.__shuffle:\n",
    "                np.random.shuffle(indicies)\n",
    "\n",
    "            while True:\n",
    "                for start_index in range(0, maximum_batch_size, self.__batch_size):\n",
    "                    splice = indicies[start_index : start_index + self.__batch_size]\n",
    "                    input_data = np.array([inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "                    output_data = outputs[splice + self.__offset].reshape(-1, 1)\n",
    "\n",
    "                    yield input_data, output_data\n",
    "                    \n",
    "        # Skip rows where needed to allow data to be loaded properly when there is not enough memory.\n",
    "        if (self.__total_size >= self.__ram_threshold):\n",
    "            number_of_chunks = np.arange(self.__total_size / self.__chunk_size)\n",
    "            if self.__shuffle:\n",
    "                np.random.shuffle(number_of_chunks)\n",
    "\n",
    "            # Yield the data in sections.\n",
    "            for index in number_of_chunks:\n",
    "                data_array = np.array(pd.read_csv(self.__file_name, skiprows=int(index) * self.__chunk_size, header=0, nrows=self.__crop, skip_blank_lines=True))                   \n",
    "                inputs = data_array[:, 0]\n",
    "                outputs = data_array[:, 1]\n",
    "\n",
    "                maximum_batch_size = inputs.size - 2 * self.__offset\n",
    "                self.__total_num_samples = maximum_batch_size\n",
    "                if self.__batch_size < 0:\n",
    "                    self.__batch_size = maximum_batch_size\n",
    "\n",
    "                indicies = np.arange(maximum_batch_size)\n",
    "                if self.__shuffle:\n",
    "                    np.random.shuffle(indicies)\n",
    "\n",
    "            while True:\n",
    "                for start_index in range(0, maximum_batch_size, self.__batch_size):\n",
    "                    splice = indicies[start_index : start_index + self.__batch_size]\n",
    "                    input_data = np.array([inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "                    output_data = outputs[splice + self.__offset].reshape(-1, 1)\n",
    "\n",
    "                    yield input_data, output_data\n",
    "                    \n",
    "class TestSlidingWindowGenerator(object):\n",
    "\n",
    "    \"\"\"Yields features and targets for testing and validating a ConvNet.\n",
    "\n",
    "    Parameters:\n",
    "    __number_of_windows (int): The number of sliding windows to produce.\n",
    "    __offset (int): The offset of the infered value from the sliding window.\n",
    "    __inputs (numpy.ndarray): The available testing / validation features.\n",
    "    __targets (numpy.ndarray): The target values corresponding to __inputs.\n",
    "    __total_size (int): The total number of inputs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_windows, inputs, targets, offset):\n",
    "        self.__number_of_windows = number_of_windows\n",
    "        self.__offset = offset\n",
    "        self.__inputs = inputs\n",
    "        self.__targets = targets\n",
    "        self.total_size = len(inputs)\n",
    "\n",
    "    def load_dataset(self):\n",
    "\n",
    "        \"\"\"Yields features and targets for testing and validating a ConvNet.\n",
    "\n",
    "        Yields:\n",
    "        input_data (numpy.array): An array of features to test / validate the network with.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.__inputs = self.__inputs.flatten()\n",
    "        max_number_of_windows = self.__inputs.size - 2 * self.__offset\n",
    "\n",
    "        if self.__number_of_windows < 0:\n",
    "            self.__number_of_windows = max_number_of_windows\n",
    "\n",
    "        indicies = np.arange(max_number_of_windows, dtype=int)\n",
    "        for start_index in range(0, max_number_of_windows, self.__number_of_windows):\n",
    "            splice = indicies[start_index : start_index + self.__number_of_windows]\n",
    "            input_data = np.array([self.__inputs[index : index + 2 * self.__offset + 1] for index in splice])\n",
    "            target_data = self.__targets[splice + self.__offset].reshape(-1, 1)\n",
    "            yield input_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb862de",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ac4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "\n",
    "def create_model(input_window_length):\n",
    "\n",
    "    \"\"\"Specifies the structure of a seq2point model using Keras' functional API.\n",
    "\n",
    "    Returns:\n",
    "    model (tensorflow.keras.Model): The uncompiled seq2point model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_window_length,))\n",
    "    reshape_layer = tf.keras.layers.Reshape((1, input_window_length, 1))(input_layer)\n",
    "    conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(reshape_layer)\n",
    "    conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_1)\n",
    "    conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_2)\n",
    "    conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_3)\n",
    "    conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=\"same\", activation=\"relu\")(conv_layer_4)\n",
    "    flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\n",
    "    label_layer = tf.keras.layers.Dense(1024, activation=\"relu\")(flatten_layer)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation=\"linear\")(label_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def save_model(model, network_type, algorithm, appliance, save_model_dir):\n",
    "\n",
    "    \"\"\" Saves a model to a specified location. Models are named using a combination of their \n",
    "    target appliance, architecture, and pruning algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The Keras model to save.\n",
    "    network_type (string): The architecture of the model ('', 'reduced', 'dropout', or 'reduced_dropout').\n",
    "    algorithm (string): The pruning algorithm applied to the model.\n",
    "    appliance (string): The appliance the model was trained with.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #model_path = \"saved_models/\" + appliance + \"_\" + algorithm + \"_\" + network_type + \"_model.h5\"\n",
    "    model_path = save_model_dir\n",
    "\n",
    "    if not os.path.exists (model_path):\n",
    "        open((model_path), 'a').close()\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "def load_model(model, network_type, algorithm, appliance, saved_model_dir):\n",
    "\n",
    "    \"\"\" Loads a model from a specified location.\n",
    "\n",
    "    Parameters:\n",
    "    model (tensorflow.keras.Model): The Keas model to which the loaded weights will be applied to.\n",
    "    network_type (string): The architecture of the model ('', 'reduced', 'dropout', or 'reduced_dropout').\n",
    "    algorithm (string): The pruning algorithm applied to the model.\n",
    "    appliance (string): The appliance the model was trained with.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #model_name = \"saved_models/\" + appliance + \"_\" + algorithm + \"_\" + network_type + \"_model.h5\"\n",
    "    model_name = saved_model_dir\n",
    "    print(\"PATH NAME: \", model_name)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    num_of_weights = model.count_params()\n",
    "    print(\"Loaded model with \", str(num_of_weights), \" weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac96e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(string):\n",
    "    return string.replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6d7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "class Trainer():\n",
    "\n",
    "    \"\"\" Used to train a seq2point model with or without pruning applied Supports \n",
    "    various alternative architectures. \n",
    "    \n",
    "    Parameters:\n",
    "    __appliance (string): The target appliance.\n",
    "    __network_type (string): The architecture of the model.\n",
    "    __batch_size (int): The number of rows per testing batch.\n",
    "    __window_size (int): The size of eaech sliding window\n",
    "    __window_offset (int): The offset of the inferred value from the sliding window.\n",
    "    __max_chunk_size (int): The largest possible number of row per chunk.\n",
    "    __validation_frequency (int): The number of epochs between model validation.\n",
    "    __training_directory (string): The directory of the model's training file.\n",
    "    __validation_directory (string): The directory of the model's validation file.\n",
    "    __training_chunker (TrainSlidingWindowGenerator): A sliding window provider \n",
    "    that returns feature / target pairs. For training use only.\n",
    "    __validation_chunker (TrainSlidingWindowGenerator): A sliding window provider \n",
    "    that returns feature / target pairs. For validation use only.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, appliance, batch_size, crop, network_type, \n",
    "                 training_directory, validation_directory, save_model_dir,\n",
    "                 epochs=10, input_window_length=599, validation_frequency = 1,\n",
    "                 patience=3, min_delta=1e-6, verbose=1):\n",
    "        self.__appliance = appliance\n",
    "        self.__algorithm = network_type\n",
    "        self.__network_type = network_type\n",
    "        self.__crop = crop\n",
    "        self.__batch_size = batch_size\n",
    "        self.__epochs = epochs\n",
    "        self.__patience = patience\n",
    "        self.__min_delta = min_delta\n",
    "        self.__verbose = verbose\n",
    "        self.__loss = \"mse\"\n",
    "        self.__metrics = [\"mse\", \"msle\", \"mae\"]\n",
    "        self.__learning_rate = 0.001\n",
    "        self.__beta_1=0.9\n",
    "        self.__beta_2=0.999\n",
    "        self.__save_model_dir = save_model_dir\n",
    "\n",
    "        self.__input_window_length = input_window_length\n",
    "        self.__window_size = 2+self.__input_window_length\n",
    "        self.__window_offset = int((0.5 * self.__window_size) - 1)\n",
    "        self.__max_chunk_size = 5 * 10 ** 2\n",
    "        self.__validation_frequency = validation_frequency\n",
    "        self.__ram_threshold=5*10**5\n",
    "        self.__skip_rows_train=10000000\n",
    "        self.__validation_steps=100\n",
    "        self.__skip_rows_val = 0\n",
    "\n",
    "        # Directories of the training and validation files. Always has the structure \n",
    "        # ./dataset_management/refit/{appliance_name}/{appliance_name}_training_.csv for training or \n",
    "        # ./dataset_management/refit/{appliance_name}/{appliance_name}_validation_.csv\n",
    "        self.__training_directory = training_directory\n",
    "        self.__validation_directory = validation_directory\n",
    "\n",
    "        self.__training_chunker = TrainSlidingWindowGenerator(file_name=self.__training_directory, \n",
    "                                        chunk_size=self.__max_chunk_size, \n",
    "                                        batch_size=self.__batch_size, \n",
    "                                        crop=self.__crop, shuffle=True,\n",
    "                                        skip_rows=self.__skip_rows_train, \n",
    "                                        offset=self.__window_offset, \n",
    "                                        ram_threshold=self.__ram_threshold)\n",
    "        self.__validation_chunker = TrainSlidingWindowGenerator(file_name=self.__validation_directory, \n",
    "                                            chunk_size=self.__max_chunk_size, \n",
    "                                            batch_size=self.__batch_size, \n",
    "                                            crop=self.__crop, \n",
    "                                            shuffle=True,\n",
    "                                            skip_rows=self.__skip_rows_val, \n",
    "                                            offset=self.__window_offset, \n",
    "                                            ram_threshold=self.__ram_threshold)\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        \"\"\" Trains an energy disaggregation model using a user-selected pruning algorithm (default is no pruning). \n",
    "        Plots and saves the resulting model. \"\"\"\n",
    "\n",
    "        # Calculate the optimum steps per epoch.\n",
    "        # self.__training_chunker.check_if_chunking()\n",
    "        #steps_per_training_epoch = np.round(int(self.__training_chunker.total_size / self.__batch_size), decimals=0)\n",
    "        steps_per_training_epoch = np.round(int(self.__training_chunker.total_num_samples / self.__batch_size), decimals=0)\n",
    "        \n",
    "        model = create_model(self.__input_window_length)\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.__learning_rate, beta_1=self.__beta_1, beta_2=self.__beta_2), loss=self.__loss, metrics=self.__metrics) \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=self.__min_delta, patience=self.__patience, verbose=self.__verbose, mode=\"auto\")\n",
    "\n",
    "        ## can use checkpoint ###############################################\n",
    "        # checkpoint_filepath = \"checkpoint/housedata/refit/\"+ self.__appliance + \"/\"\n",
    "        # model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        #     filepath = checkpoint_filepath,\n",
    "        #     monitor='val_loss',\n",
    "        #     verbose=0,\n",
    "        #     save_best_only=True,\n",
    "        #     save_weights_only=False,\n",
    "        #     mode='auto',\n",
    "        #     save_freq='epoch')        \n",
    "        #callbacks=[early_stopping, model_checkpoint_callback]\n",
    "        ###################################################################\n",
    "\n",
    "        callbacks=[early_stopping]\n",
    "        \n",
    "        training_history = self.default_train(model, callbacks, steps_per_training_epoch)\n",
    "\n",
    "        training_history.history[\"val_loss\"] = np.repeat(training_history.history[\"val_loss\"], self.__validation_frequency)\n",
    "\n",
    "        model.summary()\n",
    "        save_model(model, self.__network_type, self.__algorithm, \n",
    "                   self.__appliance, self.__save_model_dir)\n",
    "\n",
    "        self.plot_training_results(training_history)\n",
    "\n",
    "    def default_train(self, model, callbacks, steps_per_training_epoch):\n",
    "\n",
    "        \"\"\" The default training method the neural network will use. No pruning occurs.\n",
    "\n",
    "        Parameters:\n",
    "        model (tensorflow.keras.Model): The seq2point model being trained.\n",
    "        early_stopping (tensorflow.keras.callbacks.EarlyStopping): An early stopping callback to \n",
    "        prevent overfitting.\n",
    "        steps_per_training_epoch (int): The number of training steps to occur per epoch.\n",
    "\n",
    "        Returns:\n",
    "        training_history (numpy.ndarray): The error metrics and loss values that were calculated \n",
    "        at the end of each training epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        # ########### this is retired ##############################\n",
    "        # training_history = model.fit_generator(self.__training_chunker.load_dataset(),\n",
    "        #     steps_per_epoch=steps_per_training_epoch,\n",
    "        #     epochs=1,\n",
    "        #     verbose=1,\n",
    "        #     validation_data = self.__validation_chunker.load_dataset(),\n",
    "        #     validation_steps=100,\n",
    "        #     validation_freq=self.__validation_frequency,\n",
    "        #     callbacks=[early_stopping])\n",
    "        ############################################################\n",
    "\n",
    "        training_history = model.fit(self.__training_chunker.load_dataset(),                            \n",
    "                      steps_per_epoch=int(steps_per_training_epoch),\n",
    "                      epochs=self.__epochs,\n",
    "                      verbose=self.__verbose,\n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=self.__validation_chunker.load_dataset(),\n",
    "                      validation_freq=self.__validation_frequency,\n",
    "                      validation_steps=self.__validation_steps)\n",
    "\n",
    "        return training_history\n",
    "\n",
    "    def plot_training_results(self, training_history):\n",
    "\n",
    "        \"\"\" Plots and saves a graph of training loss against epoch.\n",
    "\n",
    "        Parameters:\n",
    "        training_history (numpy.ndarray): A timeseries of loss against epoch count.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        plt.plot(training_history.history[\"loss\"], label=\"MSE (Training Loss)\")\n",
    "        plt.plot(training_history.history[\"val_loss\"], label=\"MSE (Validation Loss)\")\n",
    "        plt.title('Training History')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        #file_name = \"./\" + self.__appliance + \"/saved_models/\" + self.__appliance + \"_\" + self.__pruning_algorithm + \"_\" + self.__network_type + \"_training_results.png\"\n",
    "        #plt.savefig(fname=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559549f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np \n",
    "import keras\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Tester():\n",
    "\n",
    "    \"\"\" Used to test and evaluate a pre-trained seq2point model with or without pruning applied. \n",
    "    \n",
    "    Parameters:\n",
    "    __appliance (string): The target appliance.\n",
    "    __algorithm (string): The (pruning) algorithm the model was trained with.\n",
    "    __network_type (string): The architecture of the model.\n",
    "    __crop (int): The maximum number of rows of data to evaluate the model with.\n",
    "    __batch_size (int): The number of rows per testing batch.\n",
    "    __window_size (int): The size of eaech sliding window\n",
    "    __window_offset (int): The offset of the inferred value from the sliding window.\n",
    "    __test_directory (string): The directory of the test file for the model.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, appliance, algorithm, crop, batch_size, network_type,\n",
    "                 test_directory, saved_model_dir, log_file_dir,\n",
    "                 input_window_length):\n",
    "        self.__appliance = appliance\n",
    "        self.__algorithm = algorithm\n",
    "        self.__network_type = network_type\n",
    "\n",
    "        self.__crop = crop\n",
    "        self.__batch_size = batch_size\n",
    "        self._input_window_length = input_window_length\n",
    "        self.__window_size = self._input_window_length + 2\n",
    "        self.__window_offset = int(0.5 * self.__window_size - 1)\n",
    "        self.__number_of_windows = 100\n",
    "\n",
    "        self.__test_directory = test_directory\n",
    "        self.__saved_model_dir = saved_model_dir\n",
    "\n",
    "        self.__log_file = log_file_dir\n",
    "        logging.basicConfig(filename=self.__log_file,level=logging.INFO)\n",
    "\n",
    "    def test_model(self):\n",
    "\n",
    "        \"\"\" Tests a fully-trained model using a sliding window generator as an input. Measures inference time, gathers, and \n",
    "        plots evaluationg metrics. \"\"\"\n",
    "\n",
    "        test_input, test_target = self.load_dataset(self.__test_directory)\n",
    "        model = create_model(self._input_window_length)\n",
    "        model = load_model(model, self.__network_type, self.__algorithm, \n",
    "                           self.__appliance, self.__saved_model_dir)\n",
    "\n",
    "        test_generator = TestSlidingWindowGenerator(number_of_windows=self.__number_of_windows, inputs=test_input, targets=test_target, offset=self.__window_offset)\n",
    "\n",
    "        # Calculate the optimum steps per epoch.\n",
    "        steps_per_test_epoch = np.round(int(test_generator.total_size / self.__batch_size), decimals=0)\n",
    "\n",
    "        # Test the model.\n",
    "        start_time = time.time()\n",
    "        testing_history = model.predict(x=test_generator.load_dataset(), steps=steps_per_test_epoch, verbose=2)\n",
    "\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time\n",
    "\n",
    "        evaluation_metrics = model.evaluate(x=test_generator.load_dataset(), steps=steps_per_test_epoch)\n",
    "\n",
    "        self.log_results(model, test_time, evaluation_metrics)\n",
    "        self.plot_results(testing_history, test_input, test_target)\n",
    "\n",
    "\n",
    "    def load_dataset(self, directory):\n",
    "        \"\"\"Loads the testing dataset from the location specified by file_name.\n",
    "\n",
    "        Parameters:\n",
    "        directory (string): The location at which the dataset is stored, concatenated with the file name.\n",
    "\n",
    "        Returns:\n",
    "        test_input (numpy.array): The first n (crop) features of the test dataset.\n",
    "        test_target (numpy.array): The first n (crop) targets of the test dataset.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data_frame = pd.read_csv(directory, nrows=self.__crop, skiprows=0, header=0)\n",
    "        test_input = np.round(np.array(data_frame.iloc[:, 0], float), 6)\n",
    "        test_target = np.round(np.array(data_frame.iloc[self.__window_offset: -self.__window_offset, 1], float), 6)\n",
    "        \n",
    "        del data_frame\n",
    "        return test_input, test_target\n",
    "\n",
    "    def log_results(self, model, test_time, evaluation_metrics):\n",
    "\n",
    "        \"\"\"Logs the inference time, MAE and MSE of an evaluated model.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The evaluated model.\n",
    "        test_time (float): The time taken by the model to infer all required values.\n",
    "        evaluation metrics (list): The MSE, MAE, and various compression ratios of the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        inference_log = \"Inference Time: \" + str(test_time)\n",
    "        logging.info(inference_log)\n",
    "\n",
    "        metric_string = \"MSE: \", str(evaluation_metrics[0]), \" MAE: \", str(evaluation_metrics[3])\n",
    "        logging.info(metric_string)\n",
    "\n",
    "        self.count_pruned_weights(model)  \n",
    "\n",
    "    def count_pruned_weights(self, model):\n",
    "\n",
    "        \"\"\" Counts the total number of weights, pruned weights, and weights in convolutional \n",
    "        layers. Calculates the sparsity ratio of different layer types and logs these values.\n",
    "\n",
    "        Parameters:\n",
    "        model (tf.keras.Model): The evaluated model.\n",
    "\n",
    "        \"\"\"\n",
    "        num_total_zeros = 0\n",
    "        num_dense_zeros = 0\n",
    "        num_dense_weights = 0\n",
    "        num_conv_zeros = 0\n",
    "        num_conv_weights = 0\n",
    "        for layer in model.layers:\n",
    "            if np.shape(layer.get_weights())[0] != 0:\n",
    "                layer_weights = layer.get_weights()[0].flatten()\n",
    "\n",
    "                if \"conv\" in layer.name:\n",
    "                    num_conv_weights += np.size(layer_weights)\n",
    "                    num_conv_zeros += np.count_nonzero(layer_weights==0)\n",
    "\n",
    "                    num_total_zeros += np.size(layer_weights)\n",
    "                else:\n",
    "                    num_dense_weights += np.size(layer_weights)\n",
    "                    num_dense_zeros += np.count_nonzero(layer_weights==0)\n",
    "\n",
    "        conv_zeros_string = \"CONV. ZEROS: \" + str(num_conv_zeros)\n",
    "        conv_weights_string = \"CONV. WEIGHTS: \" + str(num_conv_weights)\n",
    "        conv_sparsity_ratio = \"CONV. RATIO: \" + str(num_conv_zeros / num_conv_weights)\n",
    "\n",
    "        dense_weights_string = \"DENSE WEIGHTS: \" + str(num_dense_weights)\n",
    "        dense_zeros_string = \"DENSE ZEROS: \" + str(num_dense_zeros)\n",
    "        dense_sparsity_ratio = \"DENSE RATIO: \" + str(num_dense_zeros / num_dense_weights)\n",
    "\n",
    "        total_zeros_string = \"TOTAL ZEROS: \" + str(num_total_zeros)\n",
    "        total_weights_string = \"TOTAL WEIGHTS: \" + str(model.count_params())\n",
    "        total_sparsity_ratio = \"TOTAL RATIO: \" + str(num_total_zeros / model.count_params())\n",
    "\n",
    "        print(\"LOGGING PATH: \", self.__log_file)\n",
    "\n",
    "        logging.info(conv_zeros_string)\n",
    "        logging.info(conv_weights_string)\n",
    "        logging.info(conv_sparsity_ratio)\n",
    "        logging.info(\"\")\n",
    "        logging.info(dense_zeros_string)\n",
    "        logging.info(dense_weights_string)\n",
    "        logging.info(dense_sparsity_ratio)\n",
    "        logging.info(\"\")\n",
    "        logging.info(total_zeros_string)\n",
    "        logging.info(total_weights_string)\n",
    "        logging.info(total_sparsity_ratio)\n",
    "\n",
    "    def plot_results(self, testing_history, test_input, test_target):\n",
    "\n",
    "        \"\"\" Generates and saves a plot of the testing history of the model against the (actual) \n",
    "        aggregate energy values and the true appliance values.\n",
    "\n",
    "        Parameters:\n",
    "        testing_history (numpy.ndarray): The series of values inferred by the model.\n",
    "        test_input (numpy.ndarray): The aggregate energy data.\n",
    "        test_target (numpy.ndarray): The true energy values of the appliance.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        testing_history = ((testing_history * params_appliance[self.__appliance][\"std\"]) + params_appliance[self.__appliance][\"mean\"])\n",
    "        test_target = ((test_target * params_appliance[self.__appliance][\"std\"]) + params_appliance[self.__appliance][\"mean\"])\n",
    "        test_agg = (test_input.flatten() * mains_data[\"std\"]) + mains_data[\"mean\"]\n",
    "        test_agg = test_agg[:testing_history.size]\n",
    "\n",
    "        # Can't have negative energy readings - set any results below 0 to 0.\n",
    "        test_target[test_target < 0] = 0\n",
    "        testing_history[testing_history < 0] = 0\n",
    "        test_input[test_input < 0] = 0\n",
    "\n",
    "        # Plot testing outcomes against ground truth.\n",
    "        plt.figure(1)\n",
    "        plt.plot(test_agg[self.__window_offset: -self.__window_offset], label=\"Aggregate\")\n",
    "        plt.plot(test_target[:test_agg.size - (2 * self.__window_offset)], label=\"Ground Truth\")\n",
    "        plt.plot(testing_history[:test_agg.size - (2 * self.__window_offset)], label=\"Predicted\")\n",
    "        plt.title(self.__appliance + \" \" + self.__network_type + \"(\" + self.__algorithm + \")\")\n",
    "        plt.ylabel(\"Power Value (Watts)\")\n",
    "        plt.xlabel(\"Testing Window\")\n",
    "        plt.legend()\n",
    "\n",
    "        #file_path = \"./\" + self.__appliance + \"/saved_models/\" + self.__appliance + \"_\" + self.__algorithm + \"_\" + self.__network_type + \"_test_figure.png\"\n",
    "        #plt.savefig(fname=file_path)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bd317b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing training file...\n",
      "Counting number of rows...\n",
      "Done.\n",
      "The dataset contains  10000  rows\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_6/dense_12/MatMul' defined at (most recent call last):\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11864\\1812469371.py\", line 23, in <module>\n      trainer.train_model()\n    File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11864\\3301533869.py\", line 111, in train_model\n      training_history = self.default_train(model, callbacks, steps_per_training_epoch)\n    File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11864\\3301533869.py\", line 147, in default_train\n      training_history = model.fit(self.__training_chunker.load_dataset(),\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 241, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'model_6/dense_12/MatMul'\nOOM when allocating tensor with shape[1000,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_6/dense_12/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_3583]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 23\u001b[0m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      7\u001b[0m     appliance\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mappliance_name,\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#verbose=1\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 111\u001b[0m, in \u001b[0;36mTrainer.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m## can use checkpoint ###############################################\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# checkpoint_filepath = \"checkpoint/housedata/refit/\"+ self.__appliance + \"/\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m#callbacks=[early_stopping, model_checkpoint_callback]\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m###################################################################\u001b[39;00m\n\u001b[0;32m    109\u001b[0m callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[1;32m--> 111\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_training_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m training_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(training_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__validation_frequency)\n\u001b[0;32m    115\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[7], line 147\u001b[0m, in \u001b[0;36mTrainer.default_train\u001b[1;34m(self, model, callbacks, steps_per_training_epoch)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" The default training method the neural network will use. No pruning occurs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# ########### this is retired ##############################\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# training_history = model.fit_generator(self.__training_chunker.load_dataset(),\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m#     steps_per_epoch=steps_per_training_epoch,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m#     callbacks=[early_stopping])\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m############################################################\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__training_chunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m              \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps_per_training_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validation_chunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validation_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__validation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_history\n",
      "File \u001b[1;32m~\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_6/dense_12/MatMul' defined at (most recent call last):\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11864\\1812469371.py\", line 23, in <module>\n      trainer.train_model()\n    File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11864\\3301533869.py\", line 111, in train_model\n      training_history = self.default_train(model, callbacks, steps_per_training_epoch)\n    File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11864\\3301533869.py\", line 147, in default_train\n      training_history = model.fit(self.__training_chunker.load_dataset(),\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\HP\\anaconda3_1\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 241, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'model_6/dense_12/MatMul'\nOOM when allocating tensor with shape[1000,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_6/dense_12/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_3583]"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "# Allows a model to be trained from the terminal.\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    appliance= args.appliance_name,\n",
    "    batch_size=args.batch_size,\n",
    "    crop=args.crop,\n",
    "    network_type=args.network_type,\n",
    "    training_directory = training_directory,\n",
    "    validation_directory = validation_directory,\n",
    "    save_model_dir=save_model_dir,\n",
    "    epochs=args.epochs,\n",
    "    input_window_length=args.input_window_length,\n",
    "    validation_frequency=args.validation_frequency\n",
    "    #patience=3,\n",
    "    #min_delta= 1e-6,\n",
    "    #verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f025263a-a2c6-49e4-b5d9-7651e341e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"ukdale_seq2point/kettle/kettle_training_.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File exists!\")\n",
    "else:\n",
    "    print(\"File does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ae6bfe-dfaf-466f-a649-dfc7e2282dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ukdale_seq2point/kettle/kettle_training_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5c8a954-b249-4ecd-b283-1bef9ae53548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(training_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49eb9fb7-ffcf-4eac-b323-e3e314a0b4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0945945945945946</th>\n",
       "      <th>-0.699</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073710</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095823</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078624</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072482</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712133</th>\n",
       "      <td>-0.417690</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712134</th>\n",
       "      <td>-0.415848</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712135</th>\n",
       "      <td>-0.417690</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712136</th>\n",
       "      <td>-0.410319</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712137</th>\n",
       "      <td>-0.409705</td>\n",
       "      <td>-0.699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712138 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.0945945945945946  -0.699\n",
       "0                 0.073710  -0.699\n",
       "1                 0.095823  -0.699\n",
       "2                 0.078624  -0.699\n",
       "3                 0.090909  -0.699\n",
       "4                 0.072482  -0.699\n",
       "...                    ...     ...\n",
       "712133           -0.417690  -0.699\n",
       "712134           -0.415848  -0.699\n",
       "712135           -0.417690  -0.699\n",
       "712136           -0.410319  -0.699\n",
       "712137           -0.409705  -0.699\n",
       "\n",
       "[712138 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338191bf-6149-4ca3-884c-ef7a9b648416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
